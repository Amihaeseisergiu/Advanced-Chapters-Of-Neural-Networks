{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5057a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "References:\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "https://arxiv.org/pdf/1604.04112v4.pdf\n",
    "https://github.com/yu4u/cutout-random-erasing\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Add, Dense, Conv2D, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, AveragePooling2D, Input, Flatten\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e34ed2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 180\n",
    "n_classes = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c65c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb79e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet:\n",
    "    def __init__(self, size=44, stacks=3, starting_filter=16):\n",
    "        self.size = size\n",
    "        self.stacks = stacks\n",
    "        self.starting_filter = starting_filter\n",
    "        self.residual_blocks = (size - 2) // 6\n",
    "        \n",
    "    def get_model(self, input_shape=(32, 32, 3), n_classes=10):\n",
    "        n_filters = self.starting_filter\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        network = self.layer(inputs, n_filters)\n",
    "        network = self.stack(network, n_filters, True)\n",
    "\n",
    "        for _ in range(self.stacks - 1):\n",
    "            n_filters *= 2\n",
    "            network = self.stack(network, n_filters)\n",
    "\n",
    "        network = Activation('elu')(network)\n",
    "        network = AveragePooling2D(pool_size=network.shape[1])(network)\n",
    "        network = Flatten()(network)\n",
    "        outputs = Dense(n_classes, activation='softmax', \n",
    "                        kernel_initializer='he_normal')(network)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def stack(self, inputs, n_filters, first_stack=False):\n",
    "        stack = inputs\n",
    "\n",
    "        if first_stack:\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "        else:\n",
    "            stack = self.convolution_block(stack, n_filters)\n",
    "\n",
    "        for _ in range(self.residual_blocks - 1):\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "\n",
    "        return stack\n",
    "    \n",
    "    def identity_block(self, inputs, n_filters):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "\n",
    "    def convolution_block(self, inputs, n_filters, strides=2):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, strides=strides,\n",
    "                           normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        shortcut = self.layer(shortcut, n_filters,\n",
    "                              kernel_size=1, strides=strides,\n",
    "                              activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "    \n",
    "    def layer(self, inputs, n_filters, kernel_size=3,\n",
    "              strides=1, activation='elu', normalize_batch=True):\n",
    "    \n",
    "        convolution = Conv2D(n_filters, kernel_size=kernel_size,\n",
    "                             strides=strides, padding='same',\n",
    "                             kernel_initializer=\"he_normal\",\n",
    "                             kernel_regularizer=l2(1e-4))\n",
    "\n",
    "        x = convolution(inputs)\n",
    "\n",
    "        if normalize_batch:\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def learning_rate_schedule(epoch):\n",
    "    new_learning_rate = learning_rate\n",
    "\n",
    "    if epoch <= 90:\n",
    "        pass\n",
    "    elif epoch > 90 and epoch <= 140:\n",
    "        new_learning_rate = learning_rate * 0.1\n",
    "    else:\n",
    "        new_learning_rate = learning_rate * 0.01\n",
    "        \n",
    "    print('Learning rate:', new_learning_rate)\n",
    "    \n",
    "    return new_learning_rate\n",
    "\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        if input_img.ndim == 3:\n",
    "            img_h, img_w, img_c = input_img.shape\n",
    "        elif input_img.ndim == 2:\n",
    "            img_h, img_w = input_img.shape\n",
    "\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            if input_img.ndim == 3:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "            if input_img.ndim == 2:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3de5a3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2320        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2320        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           add[0][0]                        \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2320        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2320        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2320        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2320        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2320        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 16)   0           add_3[0][0]                      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 16)   2320        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 16)   2320        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 16)   0           add_4[0][0]                      \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 16)   2320        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 16)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 16)   2320        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 32, 32, 16)   0           add_5[0][0]                      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 32)   4640        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 32)   544         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 32)   9248        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 16, 16, 32)   0           batch_normalization_9[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 32)   9248        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 32)   9248        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 16, 16, 32)   0           add_7[0][0]                      \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 32)   9248        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 16, 16, 32)   0           add_8[0][0]                      \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 32)   9248        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 32)   9248        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 16, 16, 32)   0           add_9[0][0]                      \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 32)   9248        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 32)   9248        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 16, 16, 32)   0           add_10[0][0]                     \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 32)   9248        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 16, 16, 32)   0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 32)   9248        activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 32)   128         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 16, 16, 32)   0           add_11[0][0]                     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 32)   9248        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 32)   0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 16, 16, 32)   9248        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 32)   128         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 16, 16, 32)   0           add_12[0][0]                     \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 64)     18496       add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 64)     2112        add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 64)     36928       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 8, 8, 64)     0           batch_normalization_17[0][0]     \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 64)     36928       add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 64)     36928       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 8, 8, 64)     0           add_14[0][0]                     \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 64)     36928       add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 64)     36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 8, 8, 64)     0           add_15[0][0]                     \n",
      "                                                                 batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 64)     36928       add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 64)     36928       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 64)     256         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 8, 8, 64)     0           add_16[0][0]                     \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 8, 8, 64)     36928       add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 64)     0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 8, 8, 64)     36928       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 64)     256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 8, 8, 64)     0           add_17[0][0]                     \n",
      "                                                                 batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 8, 8, 64)     36928       add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 64)     0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 8, 8, 64)     36928       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 64)     256         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 8, 8, 64)     0           add_18[0][0]                     \n",
      "                                                                 batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 8, 8, 64)     36928       add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 64)     0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 8, 8, 64)     36928       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 64)     256         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 8, 8, 64)     0           add_19[0][0]                     \n",
      "                                                                 batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 64)     0           add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           650         flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 663,242\n",
      "Trainable params: 661,450\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet = Resnet()\n",
    "\n",
    "model = resnet.get_model()\n",
    "\n",
    "optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22558699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 30s 62ms/step - loss: 2.0053 - accuracy: 0.3974 - val_loss: 1.6091 - val_accuracy: 0.5333\n",
      "Epoch 2/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 1.5737 - accuracy: 0.5437 - val_loss: 1.6287 - val_accuracy: 0.5537\n",
      "Epoch 3/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 1.3549 - accuracy: 0.6206 - val_loss: 1.2497 - val_accuracy: 0.6726\n",
      "Epoch 4/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 1.2031 - accuracy: 0.6727 - val_loss: 1.2575 - val_accuracy: 0.6657\n",
      "Epoch 5/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 1.1053 - accuracy: 0.7038 - val_loss: 0.9940 - val_accuracy: 0.7425\n",
      "Epoch 6/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 1.0329 - accuracy: 0.7287 - val_loss: 1.0013 - val_accuracy: 0.7384\n",
      "Epoch 7/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.9747 - accuracy: 0.7439 - val_loss: 0.9786 - val_accuracy: 0.7468\n",
      "Epoch 8/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.9283 - accuracy: 0.7585 - val_loss: 0.8776 - val_accuracy: 0.7828\n",
      "Epoch 9/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.8923 - accuracy: 0.7690 - val_loss: 1.1401 - val_accuracy: 0.6986\n",
      "Epoch 10/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.8566 - accuracy: 0.7829 - val_loss: 0.8561 - val_accuracy: 0.7868\n",
      "Epoch 11/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.8359 - accuracy: 0.7875 - val_loss: 0.8095 - val_accuracy: 0.7983\n",
      "Epoch 12/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.8125 - accuracy: 0.7959 - val_loss: 0.7465 - val_accuracy: 0.8201\n",
      "Epoch 13/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.7944 - accuracy: 0.8024 - val_loss: 0.7449 - val_accuracy: 0.8254\n",
      "Epoch 14/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.7794 - accuracy: 0.8063 - val_loss: 0.7494 - val_accuracy: 0.8274\n",
      "Epoch 15/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.7627 - accuracy: 0.8128 - val_loss: 0.8452 - val_accuracy: 0.7933\n",
      "Epoch 16/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.7467 - accuracy: 0.8167 - val_loss: 0.6970 - val_accuracy: 0.8409\n",
      "Epoch 17/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.7396 - accuracy: 0.8215 - val_loss: 0.7384 - val_accuracy: 0.8265\n",
      "Epoch 18/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.7396 - accuracy: 0.8196 - val_loss: 0.7500 - val_accuracy: 0.8177\n",
      "Epoch 19/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.7290 - accuracy: 0.8242 - val_loss: 0.7222 - val_accuracy: 0.8303\n",
      "Epoch 20/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.7099 - accuracy: 0.8326 - val_loss: 0.6873 - val_accuracy: 0.8468\n",
      "Epoch 21/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.7053 - accuracy: 0.8335 - val_loss: 0.8111 - val_accuracy: 0.8000\n",
      "Epoch 22/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.7054 - accuracy: 0.8340 - val_loss: 0.6862 - val_accuracy: 0.8465\n",
      "Epoch 23/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.6891 - accuracy: 0.8408 - val_loss: 0.7287 - val_accuracy: 0.8370\n",
      "Epoch 24/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.6920 - accuracy: 0.8388 - val_loss: 0.7919 - val_accuracy: 0.8229\n",
      "Epoch 25/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.6935 - accuracy: 0.8413 - val_loss: 0.6309 - val_accuracy: 0.8651\n",
      "Epoch 26/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 67ms/step - loss: 0.6817 - accuracy: 0.8464 - val_loss: 0.6785 - val_accuracy: 0.8543\n",
      "Epoch 27/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 67ms/step - loss: 0.6820 - accuracy: 0.8447 - val_loss: 0.7048 - val_accuracy: 0.8451\n",
      "Epoch 28/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6795 - accuracy: 0.8454 - val_loss: 0.6631 - val_accuracy: 0.8527\n",
      "Epoch 29/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6769 - accuracy: 0.8482 - val_loss: 0.6212 - val_accuracy: 0.8701\n",
      "Epoch 30/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6726 - accuracy: 0.8505 - val_loss: 0.7963 - val_accuracy: 0.8283\n",
      "Epoch 31/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6689 - accuracy: 0.8537 - val_loss: 0.6636 - val_accuracy: 0.8648\n",
      "Epoch 32/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6665 - accuracy: 0.8541 - val_loss: 0.6674 - val_accuracy: 0.8600\n",
      "Epoch 33/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.6645 - accuracy: 0.8545 - val_loss: 0.7300 - val_accuracy: 0.8372\n",
      "Epoch 34/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.6650 - accuracy: 0.8560 - val_loss: 0.6217 - val_accuracy: 0.8753\n",
      "Epoch 35/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 65ms/step - loss: 0.6560 - accuracy: 0.8575 - val_loss: 0.7643 - val_accuracy: 0.8235\n",
      "Epoch 36/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6557 - accuracy: 0.8603 - val_loss: 0.6915 - val_accuracy: 0.8520\n",
      "Epoch 37/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6484 - accuracy: 0.8617 - val_loss: 0.7163 - val_accuracy: 0.8421\n",
      "Epoch 38/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6519 - accuracy: 0.8631 - val_loss: 0.6156 - val_accuracy: 0.8775\n",
      "Epoch 39/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6571 - accuracy: 0.8598 - val_loss: 0.7184 - val_accuracy: 0.8447\n",
      "Epoch 40/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6549 - accuracy: 0.8611 - val_loss: 0.6528 - val_accuracy: 0.8734\n",
      "Epoch 41/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6473 - accuracy: 0.8640 - val_loss: 0.7041 - val_accuracy: 0.8501\n",
      "Epoch 42/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6505 - accuracy: 0.8631 - val_loss: 0.7047 - val_accuracy: 0.8551\n",
      "Epoch 43/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6445 - accuracy: 0.8651 - val_loss: 0.6332 - val_accuracy: 0.8745\n",
      "Epoch 44/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.6428 - accuracy: 0.8660 - val_loss: 1.0287 - val_accuracy: 0.7425\n",
      "Epoch 45/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.6501 - accuracy: 0.8638 - val_loss: 0.7711 - val_accuracy: 0.8368\n",
      "Epoch 46/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6474 - accuracy: 0.8647 - val_loss: 0.6155 - val_accuracy: 0.8835\n",
      "Epoch 47/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.6422 - accuracy: 0.8674 - val_loss: 0.6676 - val_accuracy: 0.8653\n",
      "Epoch 48/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6439 - accuracy: 0.8681 - val_loss: 0.7578 - val_accuracy: 0.8513\n",
      "Epoch 49/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.6416 - accuracy: 0.8687 - val_loss: 0.6194 - val_accuracy: 0.8823\n",
      "Epoch 50/180\n",
      "Learning rate: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 26s 66ms/step - loss: 0.6402 - accuracy: 0.8689 - val_loss: 0.6161 - val_accuracy: 0.8846\n",
      "Epoch 51/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6409 - accuracy: 0.8681 - val_loss: 0.6798 - val_accuracy: 0.8635\n",
      "Epoch 52/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6313 - accuracy: 0.8747 - val_loss: 0.7897 - val_accuracy: 0.8251\n",
      "Epoch 53/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6359 - accuracy: 0.8701 - val_loss: 0.8079 - val_accuracy: 0.8258\n",
      "Epoch 54/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6296 - accuracy: 0.8728 - val_loss: 0.8144 - val_accuracy: 0.8262\n",
      "Epoch 55/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6281 - accuracy: 0.8755 - val_loss: 1.3317 - val_accuracy: 0.6330\n",
      "Epoch 56/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6307 - accuracy: 0.8729 - val_loss: 0.6510 - val_accuracy: 0.8708\n",
      "Epoch 57/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.6349 - accuracy: 0.8719 - val_loss: 0.6238 - val_accuracy: 0.8846\n",
      "Epoch 58/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.6320 - accuracy: 0.8725 - val_loss: 0.7208 - val_accuracy: 0.8489\n",
      "Epoch 59/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.6305 - accuracy: 0.8755 - val_loss: 0.6626 - val_accuracy: 0.8707\n",
      "Epoch 60/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.6349 - accuracy: 0.8747 - val_loss: 0.6525 - val_accuracy: 0.8752\n",
      "Epoch 61/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.6314 - accuracy: 0.8729 - val_loss: 0.6331 - val_accuracy: 0.8741\n",
      "Epoch 62/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.6254 - accuracy: 0.8768 - val_loss: 0.7258 - val_accuracy: 0.8509\n",
      "Epoch 63/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.6290 - accuracy: 0.8761 - val_loss: 0.6618 - val_accuracy: 0.8697\n",
      "Epoch 64/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 67ms/step - loss: 0.6272 - accuracy: 0.8759 - val_loss: 0.6621 - val_accuracy: 0.8687\n",
      "Epoch 65/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6311 - accuracy: 0.8741 - val_loss: 0.6899 - val_accuracy: 0.8658\n",
      "Epoch 66/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6341 - accuracy: 0.8744 - val_loss: 0.7421 - val_accuracy: 0.8477\n",
      "Epoch 67/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6253 - accuracy: 0.8773 - val_loss: 0.7802 - val_accuracy: 0.8383\n",
      "Epoch 68/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.6202 - accuracy: 0.8769 - val_loss: 0.6978 - val_accuracy: 0.8661\n",
      "Epoch 69/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.6270 - accuracy: 0.8773 - val_loss: 1.0551 - val_accuracy: 0.7844\n",
      "Epoch 70/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.6205 - accuracy: 0.8802 - val_loss: 0.6487 - val_accuracy: 0.8799\n",
      "Epoch 71/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6296 - accuracy: 0.8769 - val_loss: 0.6210 - val_accuracy: 0.8850\n",
      "Epoch 72/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 67ms/step - loss: 0.6208 - accuracy: 0.8791 - val_loss: 0.6657 - val_accuracy: 0.8757\n",
      "Epoch 73/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.6292 - accuracy: 0.8762 - val_loss: 0.6683 - val_accuracy: 0.8731\n",
      "Epoch 74/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.6187 - accuracy: 0.8818 - val_loss: 0.6408 - val_accuracy: 0.8809\n",
      "Epoch 75/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.6227 - accuracy: 0.8797 - val_loss: 0.6858 - val_accuracy: 0.8684\n",
      "Epoch 76/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6242 - accuracy: 0.8784 - val_loss: 0.6729 - val_accuracy: 0.8739\n",
      "Epoch 77/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.6193 - accuracy: 0.8809 - val_loss: 0.6373 - val_accuracy: 0.8879\n",
      "Epoch 78/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6183 - accuracy: 0.8816 - val_loss: 0.6569 - val_accuracy: 0.8771\n",
      "Epoch 79/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6236 - accuracy: 0.8791 - val_loss: 0.6381 - val_accuracy: 0.8796\n",
      "Epoch 80/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.6246 - accuracy: 0.8794 - val_loss: 0.6366 - val_accuracy: 0.8857\n",
      "Epoch 81/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.6178 - accuracy: 0.8810 - val_loss: 0.6198 - val_accuracy: 0.8913\n",
      "Epoch 82/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6213 - accuracy: 0.8802 - val_loss: 0.7788 - val_accuracy: 0.8380\n",
      "Epoch 83/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.6223 - accuracy: 0.8799 - val_loss: 0.6485 - val_accuracy: 0.8844\n",
      "Epoch 84/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.6229 - accuracy: 0.8817 - val_loss: 0.7765 - val_accuracy: 0.8434\n",
      "Epoch 85/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6249 - accuracy: 0.8803 - val_loss: 0.6744 - val_accuracy: 0.8779\n",
      "Epoch 86/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6176 - accuracy: 0.8832 - val_loss: 0.6423 - val_accuracy: 0.8828\n",
      "Epoch 87/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6142 - accuracy: 0.8836 - val_loss: 0.6163 - val_accuracy: 0.8884\n",
      "Epoch 88/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.6171 - accuracy: 0.8828 - val_loss: 0.6642 - val_accuracy: 0.8693\n",
      "Epoch 89/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.6191 - accuracy: 0.8828 - val_loss: 0.6384 - val_accuracy: 0.8820\n",
      "Epoch 90/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.6115 - accuracy: 0.8846 - val_loss: 0.6380 - val_accuracy: 0.8823\n",
      "Epoch 91/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.6206 - accuracy: 0.8811 - val_loss: 0.6264 - val_accuracy: 0.8856\n",
      "Epoch 92/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.5328 - accuracy: 0.9136 - val_loss: 0.4897 - val_accuracy: 0.9278\n",
      "Epoch 93/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.4744 - accuracy: 0.9313 - val_loss: 0.4750 - val_accuracy: 0.9324\n",
      "Epoch 94/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.4541 - accuracy: 0.9367 - val_loss: 0.4680 - val_accuracy: 0.9355\n",
      "Epoch 95/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 67ms/step - loss: 0.4430 - accuracy: 0.9387 - val_loss: 0.4616 - val_accuracy: 0.9363\n",
      "Epoch 96/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.4324 - accuracy: 0.9419 - val_loss: 0.4573 - val_accuracy: 0.9371\n",
      "Epoch 97/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.4250 - accuracy: 0.9431 - val_loss: 0.4528 - val_accuracy: 0.9382\n",
      "Epoch 98/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.4153 - accuracy: 0.9456 - val_loss: 0.4501 - val_accuracy: 0.9346\n",
      "Epoch 99/180\n",
      "Learning rate: 0.010000000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 27s 68ms/step - loss: 0.4064 - accuracy: 0.9461 - val_loss: 0.4398 - val_accuracy: 0.9389\n",
      "Epoch 100/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.3969 - accuracy: 0.9492 - val_loss: 0.4344 - val_accuracy: 0.9399\n",
      "Epoch 101/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.3933 - accuracy: 0.9484 - val_loss: 0.4339 - val_accuracy: 0.9388\n",
      "Epoch 102/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.3835 - accuracy: 0.9506 - val_loss: 0.4302 - val_accuracy: 0.9407\n",
      "Epoch 103/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3797 - accuracy: 0.9512 - val_loss: 0.4262 - val_accuracy: 0.9401\n",
      "Epoch 104/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3713 - accuracy: 0.9534 - val_loss: 0.4250 - val_accuracy: 0.9392\n",
      "Epoch 105/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.3672 - accuracy: 0.9533 - val_loss: 0.4179 - val_accuracy: 0.9410\n",
      "Epoch 106/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.3597 - accuracy: 0.9548 - val_loss: 0.4181 - val_accuracy: 0.9405\n",
      "Epoch 107/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.3540 - accuracy: 0.9561 - val_loss: 0.4144 - val_accuracy: 0.9408\n",
      "Epoch 108/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.3497 - accuracy: 0.9553 - val_loss: 0.4071 - val_accuracy: 0.9417\n",
      "Epoch 109/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.3477 - accuracy: 0.9549 - val_loss: 0.4043 - val_accuracy: 0.9421\n",
      "Epoch 110/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.3435 - accuracy: 0.9562 - val_loss: 0.4075 - val_accuracy: 0.9401\n",
      "Epoch 111/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.3356 - accuracy: 0.9580 - val_loss: 0.4038 - val_accuracy: 0.9414\n",
      "Epoch 112/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.3309 - accuracy: 0.9586 - val_loss: 0.3954 - val_accuracy: 0.9398\n",
      "Epoch 113/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.3292 - accuracy: 0.9582 - val_loss: 0.3957 - val_accuracy: 0.9408\n",
      "Epoch 114/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.3259 - accuracy: 0.9583 - val_loss: 0.3977 - val_accuracy: 0.9411\n",
      "Epoch 115/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.3209 - accuracy: 0.9591 - val_loss: 0.3950 - val_accuracy: 0.9395\n",
      "Epoch 116/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.3148 - accuracy: 0.9589 - val_loss: 0.3937 - val_accuracy: 0.9403\n",
      "Epoch 117/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.3160 - accuracy: 0.9591 - val_loss: 0.3904 - val_accuracy: 0.9378\n",
      "Epoch 118/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 65ms/step - loss: 0.3058 - accuracy: 0.9616 - val_loss: 0.3875 - val_accuracy: 0.9420\n",
      "Epoch 119/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.3029 - accuracy: 0.9631 - val_loss: 0.3832 - val_accuracy: 0.9400\n",
      "Epoch 120/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.3007 - accuracy: 0.9621 - val_loss: 0.3783 - val_accuracy: 0.9423\n",
      "Epoch 121/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2985 - accuracy: 0.9621 - val_loss: 0.3809 - val_accuracy: 0.9393\n",
      "Epoch 122/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2970 - accuracy: 0.9609 - val_loss: 0.3772 - val_accuracy: 0.9380\n",
      "Epoch 123/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2899 - accuracy: 0.9633 - val_loss: 0.3794 - val_accuracy: 0.9409\n",
      "Epoch 124/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2930 - accuracy: 0.9606 - val_loss: 0.3799 - val_accuracy: 0.9371\n",
      "Epoch 125/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2880 - accuracy: 0.9619 - val_loss: 0.3732 - val_accuracy: 0.9402\n",
      "Epoch 126/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2852 - accuracy: 0.9626 - val_loss: 0.3713 - val_accuracy: 0.9398\n",
      "Epoch 127/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2816 - accuracy: 0.9631 - val_loss: 0.3728 - val_accuracy: 0.9394\n",
      "Epoch 128/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.2801 - accuracy: 0.9624 - val_loss: 0.3654 - val_accuracy: 0.9401\n",
      "Epoch 129/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2741 - accuracy: 0.9647 - val_loss: 0.3609 - val_accuracy: 0.9437\n",
      "Epoch 130/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2758 - accuracy: 0.9624 - val_loss: 0.3689 - val_accuracy: 0.9390\n",
      "Epoch 131/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2685 - accuracy: 0.9648 - val_loss: 0.3663 - val_accuracy: 0.9408\n",
      "Epoch 132/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2699 - accuracy: 0.9638 - val_loss: 0.3678 - val_accuracy: 0.9367\n",
      "Epoch 133/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2652 - accuracy: 0.9643 - val_loss: 0.3680 - val_accuracy: 0.9391\n",
      "Epoch 134/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2616 - accuracy: 0.9646 - val_loss: 0.3747 - val_accuracy: 0.9368\n",
      "Epoch 135/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.2652 - accuracy: 0.9636 - val_loss: 0.3680 - val_accuracy: 0.9382\n",
      "Epoch 136/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2596 - accuracy: 0.9647 - val_loss: 0.3687 - val_accuracy: 0.9374\n",
      "Epoch 137/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2583 - accuracy: 0.9638 - val_loss: 0.3808 - val_accuracy: 0.9363\n",
      "Epoch 138/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2536 - accuracy: 0.9656 - val_loss: 0.3600 - val_accuracy: 0.9389\n",
      "Epoch 139/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2531 - accuracy: 0.9658 - val_loss: 0.3669 - val_accuracy: 0.9392\n",
      "Epoch 140/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2523 - accuracy: 0.9658 - val_loss: 0.3652 - val_accuracy: 0.9393\n",
      "Epoch 141/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2483 - accuracy: 0.9655 - val_loss: 0.3718 - val_accuracy: 0.9376\n",
      "Epoch 142/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2436 - accuracy: 0.9670 - val_loss: 0.3478 - val_accuracy: 0.9420\n",
      "Epoch 143/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 67ms/step - loss: 0.2308 - accuracy: 0.9722 - val_loss: 0.3451 - val_accuracy: 0.9436\n",
      "Epoch 144/180\n",
      "Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2296 - accuracy: 0.9734 - val_loss: 0.3410 - val_accuracy: 0.9465\n",
      "Epoch 145/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 67ms/step - loss: 0.2265 - accuracy: 0.9742 - val_loss: 0.3374 - val_accuracy: 0.9469\n",
      "Epoch 146/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2253 - accuracy: 0.9736 - val_loss: 0.3376 - val_accuracy: 0.9476\n",
      "Epoch 147/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2237 - accuracy: 0.9741 - val_loss: 0.3370 - val_accuracy: 0.9471\n",
      "Epoch 148/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2208 - accuracy: 0.9757 - val_loss: 0.3391 - val_accuracy: 0.9464\n",
      "Epoch 149/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2242 - accuracy: 0.9746 - val_loss: 0.3377 - val_accuracy: 0.9478\n",
      "Epoch 150/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 65ms/step - loss: 0.2218 - accuracy: 0.9752 - val_loss: 0.3351 - val_accuracy: 0.9467\n",
      "Epoch 151/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2204 - accuracy: 0.9757 - val_loss: 0.3362 - val_accuracy: 0.9476\n",
      "Epoch 152/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2195 - accuracy: 0.9763 - val_loss: 0.3356 - val_accuracy: 0.9460\n",
      "Epoch 153/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2186 - accuracy: 0.9758 - val_loss: 0.3359 - val_accuracy: 0.9471\n",
      "Epoch 154/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2201 - accuracy: 0.9753 - val_loss: 0.3351 - val_accuracy: 0.9469\n",
      "Epoch 155/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2202 - accuracy: 0.9751 - val_loss: 0.3346 - val_accuracy: 0.9473\n",
      "Epoch 156/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2160 - accuracy: 0.9762 - val_loss: 0.3356 - val_accuracy: 0.9474\n",
      "Epoch 157/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2183 - accuracy: 0.9759 - val_loss: 0.3333 - val_accuracy: 0.9473\n",
      "Epoch 158/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2174 - accuracy: 0.9765 - val_loss: 0.3311 - val_accuracy: 0.9472\n",
      "Epoch 159/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2161 - accuracy: 0.9758 - val_loss: 0.3300 - val_accuracy: 0.9480\n",
      "Epoch 160/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.2126 - accuracy: 0.9771 - val_loss: 0.3292 - val_accuracy: 0.9489\n",
      "Epoch 161/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2143 - accuracy: 0.9775 - val_loss: 0.3298 - val_accuracy: 0.9494\n",
      "Epoch 162/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2120 - accuracy: 0.9779 - val_loss: 0.3304 - val_accuracy: 0.9482\n",
      "Epoch 163/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2127 - accuracy: 0.9773 - val_loss: 0.3317 - val_accuracy: 0.9482\n",
      "Epoch 164/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.2131 - accuracy: 0.9766 - val_loss: 0.3297 - val_accuracy: 0.9484\n",
      "Epoch 165/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.2129 - accuracy: 0.9766 - val_loss: 0.3302 - val_accuracy: 0.9472\n",
      "Epoch 166/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2114 - accuracy: 0.9777 - val_loss: 0.3300 - val_accuracy: 0.9478\n",
      "Epoch 167/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.2150 - accuracy: 0.9757 - val_loss: 0.3305 - val_accuracy: 0.9484\n",
      "Epoch 168/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.2100 - accuracy: 0.9779 - val_loss: 0.3282 - val_accuracy: 0.9489\n",
      "Epoch 169/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2088 - accuracy: 0.9771 - val_loss: 0.3301 - val_accuracy: 0.9478\n",
      "Epoch 170/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.2101 - accuracy: 0.9771 - val_loss: 0.3301 - val_accuracy: 0.9471\n",
      "Epoch 171/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.2079 - accuracy: 0.9783 - val_loss: 0.3299 - val_accuracy: 0.9478\n",
      "Epoch 172/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2084 - accuracy: 0.9780 - val_loss: 0.3291 - val_accuracy: 0.9483\n",
      "Epoch 173/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.2077 - accuracy: 0.9778 - val_loss: 0.3292 - val_accuracy: 0.9474\n",
      "Epoch 174/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 66ms/step - loss: 0.2083 - accuracy: 0.9783 - val_loss: 0.3277 - val_accuracy: 0.9485\n",
      "Epoch 175/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2078 - accuracy: 0.9775 - val_loss: 0.3261 - val_accuracy: 0.9485\n",
      "Epoch 176/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 26s 65ms/step - loss: 0.2088 - accuracy: 0.9776 - val_loss: 0.3273 - val_accuracy: 0.9482\n",
      "Epoch 177/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2062 - accuracy: 0.9787 - val_loss: 0.3270 - val_accuracy: 0.9483\n",
      "Epoch 178/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2064 - accuracy: 0.9783 - val_loss: 0.3261 - val_accuracy: 0.9484\n",
      "Epoch 179/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2080 - accuracy: 0.9774 - val_loss: 0.3272 - val_accuracy: 0.9483\n",
      "Epoch 180/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 25s 64ms/step - loss: 0.2101 - accuracy: 0.9764 - val_loss: 0.3289 - val_accuracy: 0.9480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\acnn\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
    "callbacks = [lr_scheduler]\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=4,\n",
    "                             height_shift_range=4,\n",
    "                             horizontal_flip=True,\n",
    "                             preprocessing_function=get_random_eraser(p=1, pixel_level=True))\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=epochs, workers=4,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa6929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
