{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "5a5057a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "References:\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "https://arxiv.org/pdf/1604.04112v4.pdf\n",
    "https://github.com/yu4u/cutout-random-erasing\n",
    "\"\"\"\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.layers import Add, Dense, Conv2D, BatchNormalization\n",
    "from keras.layers import Activation, AveragePooling2D, Input, Flatten\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "e34ed2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 180\n",
    "n_classes = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "30c65c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "0eb79e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet:\n",
    "    def __init__(self, size=44, stacks=3, starting_filter=16):\n",
    "        self.size = size\n",
    "        self.stacks = stacks\n",
    "        self.starting_filter = starting_filter\n",
    "        self.residual_blocks = (size - 2) // 6\n",
    "        \n",
    "    def get_model(self, input_shape=(32, 32, 3), n_classes=10):\n",
    "        n_filters = self.starting_filter\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        network = self.layer(inputs, n_filters)\n",
    "        network = self.stack(network, n_filters, True)\n",
    "\n",
    "        for _ in range(self.stacks - 1):\n",
    "            n_filters *= 2\n",
    "            network = self.stack(network, n_filters)\n",
    "\n",
    "        network = Activation('elu')(network)\n",
    "        network = AveragePooling2D(pool_size=network.shape[1])(network)\n",
    "        network = Flatten()(network)\n",
    "        outputs = Dense(n_classes, activation='softmax', \n",
    "                        kernel_initializer='he_normal')(network)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def stack(self, inputs, n_filters, first_stack=False):\n",
    "        stack = inputs\n",
    "\n",
    "        if first_stack:\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "        else:\n",
    "            stack = self.convolution_block(stack, n_filters)\n",
    "\n",
    "        for _ in range(self.residual_blocks - 1):\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "\n",
    "        return stack\n",
    "    \n",
    "    def identity_block(self, inputs, n_filters):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "\n",
    "    def convolution_block(self, inputs, n_filters, strides=2):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, strides=strides,\n",
    "                           normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        shortcut = self.layer(shortcut, n_filters,\n",
    "                              kernel_size=1, strides=strides,\n",
    "                              activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "    \n",
    "    def layer(self, inputs, n_filters, kernel_size=3,\n",
    "              strides=1, activation='elu', normalize_batch=True):\n",
    "    \n",
    "        convolution = Conv2D(n_filters, kernel_size=kernel_size,\n",
    "                             strides=strides, padding='same',\n",
    "                             kernel_initializer=\"he_normal\",\n",
    "                             kernel_regularizer=l2(1e-4))\n",
    "\n",
    "        x = convolution(inputs)\n",
    "\n",
    "        if normalize_batch:\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def learning_rate_schedule(epoch):\n",
    "    new_learning_rate = learning_rate\n",
    "\n",
    "    if epoch <= 90:\n",
    "        pass\n",
    "    elif epoch > 90 and epoch <= 140:\n",
    "        new_learning_rate = learning_rate * 0.1\n",
    "    else:\n",
    "        new_learning_rate = learning_rate * 0.01\n",
    "        \n",
    "    print('Learning rate:', new_learning_rate)\n",
    "    \n",
    "    return new_learning_rate\n",
    "\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        if input_img.ndim == 3:\n",
    "            img_h, img_w, img_c = input_img.shape\n",
    "        elif input_img.ndim == 2:\n",
    "            img_h, img_w = input_img.shape\n",
    "\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            if input_img.ndim == 3:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "            if input_img.ndim == 2:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "3de5a3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_45\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_46 (InputLayer)           [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2013 (Conv2D)            (None, 32, 32, 16)   448         input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1462 (Batch (None, 32, 32, 16)   64          conv2d_2013[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1429 (Activation)    (None, 32, 32, 16)   0           batch_normalization_1462[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2014 (Conv2D)            (None, 32, 32, 16)   2320        activation_1429[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1430 (Activation)    (None, 32, 32, 16)   0           conv2d_2014[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2015 (Conv2D)            (None, 32, 32, 16)   2320        activation_1430[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1463 (Batch (None, 32, 32, 16)   64          conv2d_2015[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_939 (Add)                   (None, 32, 32, 16)   0           activation_1429[0][0]            \n",
      "                                                                 batch_normalization_1463[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2016 (Conv2D)            (None, 32, 32, 16)   2320        add_939[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1431 (Activation)    (None, 32, 32, 16)   0           conv2d_2016[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2017 (Conv2D)            (None, 32, 32, 16)   2320        activation_1431[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1464 (Batch (None, 32, 32, 16)   64          conv2d_2017[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_940 (Add)                   (None, 32, 32, 16)   0           add_939[0][0]                    \n",
      "                                                                 batch_normalization_1464[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2018 (Conv2D)            (None, 32, 32, 16)   2320        add_940[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1432 (Activation)    (None, 32, 32, 16)   0           conv2d_2018[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2019 (Conv2D)            (None, 32, 32, 16)   2320        activation_1432[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1465 (Batch (None, 32, 32, 16)   64          conv2d_2019[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_941 (Add)                   (None, 32, 32, 16)   0           add_940[0][0]                    \n",
      "                                                                 batch_normalization_1465[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2020 (Conv2D)            (None, 32, 32, 16)   2320        add_941[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1433 (Activation)    (None, 32, 32, 16)   0           conv2d_2020[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2021 (Conv2D)            (None, 32, 32, 16)   2320        activation_1433[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1466 (Batch (None, 32, 32, 16)   64          conv2d_2021[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_942 (Add)                   (None, 32, 32, 16)   0           add_941[0][0]                    \n",
      "                                                                 batch_normalization_1466[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2022 (Conv2D)            (None, 32, 32, 16)   2320        add_942[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1434 (Activation)    (None, 32, 32, 16)   0           conv2d_2022[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2023 (Conv2D)            (None, 32, 32, 16)   2320        activation_1434[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1467 (Batch (None, 32, 32, 16)   64          conv2d_2023[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_943 (Add)                   (None, 32, 32, 16)   0           add_942[0][0]                    \n",
      "                                                                 batch_normalization_1467[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2024 (Conv2D)            (None, 32, 32, 16)   2320        add_943[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1435 (Activation)    (None, 32, 32, 16)   0           conv2d_2024[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2025 (Conv2D)            (None, 32, 32, 16)   2320        activation_1435[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1468 (Batch (None, 32, 32, 16)   64          conv2d_2025[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_944 (Add)                   (None, 32, 32, 16)   0           add_943[0][0]                    \n",
      "                                                                 batch_normalization_1468[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2026 (Conv2D)            (None, 32, 32, 16)   2320        add_944[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1436 (Activation)    (None, 32, 32, 16)   0           conv2d_2026[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2027 (Conv2D)            (None, 32, 32, 16)   2320        activation_1436[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1469 (Batch (None, 32, 32, 16)   64          conv2d_2027[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_945 (Add)                   (None, 32, 32, 16)   0           add_944[0][0]                    \n",
      "                                                                 batch_normalization_1469[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2028 (Conv2D)            (None, 16, 16, 32)   4640        add_945[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1437 (Activation)    (None, 16, 16, 32)   0           conv2d_2028[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2030 (Conv2D)            (None, 16, 16, 32)   544         add_945[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2029 (Conv2D)            (None, 16, 16, 32)   9248        activation_1437[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1471 (Batch (None, 16, 16, 32)   128         conv2d_2030[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1470 (Batch (None, 16, 16, 32)   128         conv2d_2029[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_946 (Add)                   (None, 16, 16, 32)   0           batch_normalization_1471[0][0]   \n",
      "                                                                 batch_normalization_1470[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2031 (Conv2D)            (None, 16, 16, 32)   9248        add_946[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1438 (Activation)    (None, 16, 16, 32)   0           conv2d_2031[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2032 (Conv2D)            (None, 16, 16, 32)   9248        activation_1438[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1472 (Batch (None, 16, 16, 32)   128         conv2d_2032[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_947 (Add)                   (None, 16, 16, 32)   0           add_946[0][0]                    \n",
      "                                                                 batch_normalization_1472[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2033 (Conv2D)            (None, 16, 16, 32)   9248        add_947[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1439 (Activation)    (None, 16, 16, 32)   0           conv2d_2033[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2034 (Conv2D)            (None, 16, 16, 32)   9248        activation_1439[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1473 (Batch (None, 16, 16, 32)   128         conv2d_2034[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_948 (Add)                   (None, 16, 16, 32)   0           add_947[0][0]                    \n",
      "                                                                 batch_normalization_1473[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2035 (Conv2D)            (None, 16, 16, 32)   9248        add_948[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1440 (Activation)    (None, 16, 16, 32)   0           conv2d_2035[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2036 (Conv2D)            (None, 16, 16, 32)   9248        activation_1440[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1474 (Batch (None, 16, 16, 32)   128         conv2d_2036[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_949 (Add)                   (None, 16, 16, 32)   0           add_948[0][0]                    \n",
      "                                                                 batch_normalization_1474[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2037 (Conv2D)            (None, 16, 16, 32)   9248        add_949[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1441 (Activation)    (None, 16, 16, 32)   0           conv2d_2037[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2038 (Conv2D)            (None, 16, 16, 32)   9248        activation_1441[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1475 (Batch (None, 16, 16, 32)   128         conv2d_2038[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_950 (Add)                   (None, 16, 16, 32)   0           add_949[0][0]                    \n",
      "                                                                 batch_normalization_1475[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2039 (Conv2D)            (None, 16, 16, 32)   9248        add_950[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1442 (Activation)    (None, 16, 16, 32)   0           conv2d_2039[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2040 (Conv2D)            (None, 16, 16, 32)   9248        activation_1442[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1476 (Batch (None, 16, 16, 32)   128         conv2d_2040[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_951 (Add)                   (None, 16, 16, 32)   0           add_950[0][0]                    \n",
      "                                                                 batch_normalization_1476[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2041 (Conv2D)            (None, 16, 16, 32)   9248        add_951[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1443 (Activation)    (None, 16, 16, 32)   0           conv2d_2041[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2042 (Conv2D)            (None, 16, 16, 32)   9248        activation_1443[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1477 (Batch (None, 16, 16, 32)   128         conv2d_2042[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_952 (Add)                   (None, 16, 16, 32)   0           add_951[0][0]                    \n",
      "                                                                 batch_normalization_1477[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2043 (Conv2D)            (None, 8, 8, 64)     18496       add_952[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1444 (Activation)    (None, 8, 8, 64)     0           conv2d_2043[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2045 (Conv2D)            (None, 8, 8, 64)     2112        add_952[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2044 (Conv2D)            (None, 8, 8, 64)     36928       activation_1444[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1479 (Batch (None, 8, 8, 64)     256         conv2d_2045[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1478 (Batch (None, 8, 8, 64)     256         conv2d_2044[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_953 (Add)                   (None, 8, 8, 64)     0           batch_normalization_1479[0][0]   \n",
      "                                                                 batch_normalization_1478[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2046 (Conv2D)            (None, 8, 8, 64)     36928       add_953[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1445 (Activation)    (None, 8, 8, 64)     0           conv2d_2046[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2047 (Conv2D)            (None, 8, 8, 64)     36928       activation_1445[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1480 (Batch (None, 8, 8, 64)     256         conv2d_2047[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_954 (Add)                   (None, 8, 8, 64)     0           add_953[0][0]                    \n",
      "                                                                 batch_normalization_1480[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2048 (Conv2D)            (None, 8, 8, 64)     36928       add_954[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1446 (Activation)    (None, 8, 8, 64)     0           conv2d_2048[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2049 (Conv2D)            (None, 8, 8, 64)     36928       activation_1446[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1481 (Batch (None, 8, 8, 64)     256         conv2d_2049[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_955 (Add)                   (None, 8, 8, 64)     0           add_954[0][0]                    \n",
      "                                                                 batch_normalization_1481[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2050 (Conv2D)            (None, 8, 8, 64)     36928       add_955[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1447 (Activation)    (None, 8, 8, 64)     0           conv2d_2050[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2051 (Conv2D)            (None, 8, 8, 64)     36928       activation_1447[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1482 (Batch (None, 8, 8, 64)     256         conv2d_2051[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_956 (Add)                   (None, 8, 8, 64)     0           add_955[0][0]                    \n",
      "                                                                 batch_normalization_1482[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2052 (Conv2D)            (None, 8, 8, 64)     36928       add_956[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1448 (Activation)    (None, 8, 8, 64)     0           conv2d_2052[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2053 (Conv2D)            (None, 8, 8, 64)     36928       activation_1448[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1483 (Batch (None, 8, 8, 64)     256         conv2d_2053[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_957 (Add)                   (None, 8, 8, 64)     0           add_956[0][0]                    \n",
      "                                                                 batch_normalization_1483[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2054 (Conv2D)            (None, 8, 8, 64)     36928       add_957[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1449 (Activation)    (None, 8, 8, 64)     0           conv2d_2054[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2055 (Conv2D)            (None, 8, 8, 64)     36928       activation_1449[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1484 (Batch (None, 8, 8, 64)     256         conv2d_2055[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_958 (Add)                   (None, 8, 8, 64)     0           add_957[0][0]                    \n",
      "                                                                 batch_normalization_1484[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2056 (Conv2D)            (None, 8, 8, 64)     36928       add_958[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1450 (Activation)    (None, 8, 8, 64)     0           conv2d_2056[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2057 (Conv2D)            (None, 8, 8, 64)     36928       activation_1450[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1485 (Batch (None, 8, 8, 64)     256         conv2d_2057[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_959 (Add)                   (None, 8, 8, 64)     0           add_958[0][0]                    \n",
      "                                                                 batch_normalization_1485[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1451 (Activation)    (None, 8, 8, 64)     0           add_959[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_37 (AveragePo (None, 1, 1, 64)     0           activation_1451[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_45 (Flatten)            (None, 64)           0           average_pooling2d_37[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 10)           650         flatten_45[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 663,242\n",
      "Trainable params: 661,450\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet = Resnet()\n",
    "\n",
    "model = resnet.get_model()\n",
    "\n",
    "optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "22558699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 54ms/step - loss: 1.9110 - accuracy: 0.4273 - val_loss: 1.5964 - val_accuracy: 0.5440\n",
      "Epoch 2/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 1.4496 - accuracy: 0.5967 - val_loss: 1.4109 - val_accuracy: 0.6202\n",
      "Epoch 3/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 1.2316 - accuracy: 0.6638 - val_loss: 1.1242 - val_accuracy: 0.7089\n",
      "Epoch 4/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 1.1036 - accuracy: 0.7088 - val_loss: 1.0586 - val_accuracy: 0.7300\n",
      "Epoch 5/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 60ms/step - loss: 1.0161 - accuracy: 0.7370 - val_loss: 0.9467 - val_accuracy: 0.7602\n",
      "Epoch 6/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.9413 - accuracy: 0.7604 - val_loss: 0.8926 - val_accuracy: 0.7728\n",
      "Epoch 7/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.8892 - accuracy: 0.7781 - val_loss: 0.9725 - val_accuracy: 0.7570\n",
      "Epoch 8/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.8477 - accuracy: 0.7899 - val_loss: 0.8729 - val_accuracy: 0.7835\n",
      "Epoch 9/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.8075 - accuracy: 0.8035 - val_loss: 0.8001 - val_accuracy: 0.8107\n",
      "Epoch 10/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.7845 - accuracy: 0.8097 - val_loss: 0.7750 - val_accuracy: 0.8123\n",
      "Epoch 11/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.7513 - accuracy: 0.8203 - val_loss: 0.7431 - val_accuracy: 0.8263\n",
      "Epoch 12/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.7365 - accuracy: 0.8243 - val_loss: 0.8175 - val_accuracy: 0.8104\n",
      "Epoch 13/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.7225 - accuracy: 0.8285 - val_loss: 0.7184 - val_accuracy: 0.8383\n",
      "Epoch 14/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.7049 - accuracy: 0.8354 - val_loss: 0.7213 - val_accuracy: 0.8364\n",
      "Epoch 15/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6916 - accuracy: 0.8404 - val_loss: 0.6988 - val_accuracy: 0.8377\n",
      "Epoch 16/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6739 - accuracy: 0.8443 - val_loss: 0.7497 - val_accuracy: 0.8348\n",
      "Epoch 17/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6696 - accuracy: 0.8463 - val_loss: 0.7083 - val_accuracy: 0.8409\n",
      "Epoch 18/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6526 - accuracy: 0.8538 - val_loss: 0.8276 - val_accuracy: 0.8070\n",
      "Epoch 19/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.6515 - accuracy: 0.8547 - val_loss: 0.7111 - val_accuracy: 0.8500\n",
      "Epoch 20/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6420 - accuracy: 0.8571 - val_loss: 0.6940 - val_accuracy: 0.8514\n",
      "Epoch 21/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6342 - accuracy: 0.8606 - val_loss: 1.1521 - val_accuracy: 0.6910\n",
      "Epoch 22/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6288 - accuracy: 0.8635 - val_loss: 0.7575 - val_accuracy: 0.8310\n",
      "Epoch 23/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6193 - accuracy: 0.8675 - val_loss: 0.6819 - val_accuracy: 0.8496\n",
      "Epoch 24/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6228 - accuracy: 0.8645 - val_loss: 0.8485 - val_accuracy: 0.8118\n",
      "Epoch 25/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6183 - accuracy: 0.8685 - val_loss: 0.6835 - val_accuracy: 0.8538\n",
      "Epoch 26/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.6154 - accuracy: 0.8697 - val_loss: 0.6780 - val_accuracy: 0.8510\n",
      "Epoch 27/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6158 - accuracy: 0.8686 - val_loss: 0.6342 - val_accuracy: 0.8725\n",
      "Epoch 28/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6050 - accuracy: 0.8725 - val_loss: 0.8313 - val_accuracy: 0.8082\n",
      "Epoch 29/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6023 - accuracy: 0.8742 - val_loss: 0.7824 - val_accuracy: 0.8417\n",
      "Epoch 30/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6015 - accuracy: 0.8756 - val_loss: 0.7034 - val_accuracy: 0.8479\n",
      "Epoch 31/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5994 - accuracy: 0.8775 - val_loss: 0.7623 - val_accuracy: 0.8276\n",
      "Epoch 32/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.6020 - accuracy: 0.8773 - val_loss: 0.6900 - val_accuracy: 0.8547\n",
      "Epoch 33/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5929 - accuracy: 0.8798 - val_loss: 0.6060 - val_accuracy: 0.8796\n",
      "Epoch 34/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5918 - accuracy: 0.8804 - val_loss: 0.7248 - val_accuracy: 0.8517\n",
      "Epoch 35/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5845 - accuracy: 0.8839 - val_loss: 0.6803 - val_accuracy: 0.8616\n",
      "Epoch 36/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.5896 - accuracy: 0.8837 - val_loss: 0.6328 - val_accuracy: 0.8772\n",
      "Epoch 37/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 57ms/step - loss: 0.5851 - accuracy: 0.8835 - val_loss: 0.7035 - val_accuracy: 0.8544\n",
      "Epoch 38/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5918 - accuracy: 0.8809 - val_loss: 0.7304 - val_accuracy: 0.8510\n",
      "Epoch 39/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.5818 - accuracy: 0.8857 - val_loss: 0.6898 - val_accuracy: 0.8622\n",
      "Epoch 40/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.5863 - accuracy: 0.8858 - val_loss: 0.6045 - val_accuracy: 0.8811\n",
      "Epoch 41/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.5798 - accuracy: 0.8881 - val_loss: 0.6274 - val_accuracy: 0.8754\n",
      "Epoch 42/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.5811 - accuracy: 0.8875 - val_loss: 0.7015 - val_accuracy: 0.8506\n",
      "Epoch 43/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.5792 - accuracy: 0.8900 - val_loss: 0.6662 - val_accuracy: 0.8693\n",
      "Epoch 44/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.5770 - accuracy: 0.8887 - val_loss: 0.6896 - val_accuracy: 0.8635\n",
      "Epoch 45/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.5802 - accuracy: 0.8884 - val_loss: 0.6465 - val_accuracy: 0.8722\n",
      "Epoch 46/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.5831 - accuracy: 0.8888 - val_loss: 0.6382 - val_accuracy: 0.8777\n",
      "Epoch 47/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.5781 - accuracy: 0.8882 - val_loss: 0.6463 - val_accuracy: 0.8730\n",
      "Epoch 48/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.5645 - accuracy: 0.8928 - val_loss: 0.6548 - val_accuracy: 0.8732\n",
      "Epoch 49/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5773 - accuracy: 0.8891 - val_loss: 0.6618 - val_accuracy: 0.8742\n",
      "Epoch 50/180\n",
      "Learning rate: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 59ms/step - loss: 0.5682 - accuracy: 0.8911 - val_loss: 0.6654 - val_accuracy: 0.8648\n",
      "Epoch 51/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.5696 - accuracy: 0.8915 - val_loss: 0.6823 - val_accuracy: 0.8644\n",
      "Epoch 52/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5705 - accuracy: 0.8923 - val_loss: 0.6453 - val_accuracy: 0.8775\n",
      "Epoch 53/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5681 - accuracy: 0.8947 - val_loss: 0.6352 - val_accuracy: 0.8745\n",
      "Epoch 54/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5691 - accuracy: 0.8936 - val_loss: 0.6185 - val_accuracy: 0.8846\n",
      "Epoch 55/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.5685 - accuracy: 0.8954 - val_loss: 0.5977 - val_accuracy: 0.8963\n",
      "Epoch 56/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.5639 - accuracy: 0.8961 - val_loss: 0.6500 - val_accuracy: 0.8720\n",
      "Epoch 57/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.5620 - accuracy: 0.8958 - val_loss: 0.7780 - val_accuracy: 0.8312\n",
      "Epoch 58/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.5636 - accuracy: 0.8960 - val_loss: 1.0646 - val_accuracy: 0.7283\n",
      "Epoch 59/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.5655 - accuracy: 0.8956 - val_loss: 0.6562 - val_accuracy: 0.8790\n",
      "Epoch 60/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5662 - accuracy: 0.8957 - val_loss: 0.6849 - val_accuracy: 0.8663\n",
      "Epoch 61/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.5616 - accuracy: 0.8972 - val_loss: 0.6609 - val_accuracy: 0.8735\n",
      "Epoch 62/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5654 - accuracy: 0.8960 - val_loss: 0.6326 - val_accuracy: 0.8789\n",
      "Epoch 63/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5612 - accuracy: 0.8977 - val_loss: 0.7052 - val_accuracy: 0.8564\n",
      "Epoch 64/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5622 - accuracy: 0.8971 - val_loss: 0.6402 - val_accuracy: 0.8840\n",
      "Epoch 65/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5605 - accuracy: 0.8971 - val_loss: 0.7207 - val_accuracy: 0.8549\n",
      "Epoch 66/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.5624 - accuracy: 0.8966 - val_loss: 1.3826 - val_accuracy: 0.6144\n",
      "Epoch 67/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5526 - accuracy: 0.9022 - val_loss: 0.6497 - val_accuracy: 0.8791\n",
      "Epoch 68/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 57ms/step - loss: 0.5621 - accuracy: 0.8982 - val_loss: 0.7368 - val_accuracy: 0.8558\n",
      "Epoch 69/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5595 - accuracy: 0.8980 - val_loss: 0.6813 - val_accuracy: 0.8693\n",
      "Epoch 70/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5555 - accuracy: 0.9010 - val_loss: 0.6455 - val_accuracy: 0.8772\n",
      "Epoch 71/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5585 - accuracy: 0.8999 - val_loss: 1.5904 - val_accuracy: 0.5470\n",
      "Epoch 72/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 57ms/step - loss: 0.5631 - accuracy: 0.8981 - val_loss: 0.6723 - val_accuracy: 0.8755\n",
      "Epoch 73/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5544 - accuracy: 0.9013 - val_loss: 0.6319 - val_accuracy: 0.8826\n",
      "Epoch 74/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5565 - accuracy: 0.9007 - val_loss: 0.6470 - val_accuracy: 0.8744\n",
      "Epoch 75/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5585 - accuracy: 0.8996 - val_loss: 0.6074 - val_accuracy: 0.8841\n",
      "Epoch 76/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 57ms/step - loss: 0.5588 - accuracy: 0.9000 - val_loss: 0.6851 - val_accuracy: 0.8662\n",
      "Epoch 77/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5552 - accuracy: 0.9007 - val_loss: 0.7309 - val_accuracy: 0.8516\n",
      "Epoch 78/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5572 - accuracy: 0.9003 - val_loss: 0.6057 - val_accuracy: 0.8883\n",
      "Epoch 79/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5490 - accuracy: 0.9034 - val_loss: 1.1910 - val_accuracy: 0.7038\n",
      "Epoch 80/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5597 - accuracy: 0.8991 - val_loss: 0.6160 - val_accuracy: 0.8882\n",
      "Epoch 81/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.5572 - accuracy: 0.9013 - val_loss: 0.7212 - val_accuracy: 0.8582\n",
      "Epoch 82/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5528 - accuracy: 0.9040 - val_loss: 0.6615 - val_accuracy: 0.8784\n",
      "Epoch 83/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5595 - accuracy: 0.8987 - val_loss: 0.9757 - val_accuracy: 0.7679\n",
      "Epoch 84/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5549 - accuracy: 0.9023 - val_loss: 0.6093 - val_accuracy: 0.8917\n",
      "Epoch 85/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5568 - accuracy: 0.9009 - val_loss: 0.6691 - val_accuracy: 0.8821\n",
      "Epoch 86/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 23s 57ms/step - loss: 0.5610 - accuracy: 0.8994 - val_loss: 0.7456 - val_accuracy: 0.8430\n",
      "Epoch 87/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5533 - accuracy: 0.9026 - val_loss: 0.6415 - val_accuracy: 0.8811\n",
      "Epoch 88/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5509 - accuracy: 0.9023 - val_loss: 0.6181 - val_accuracy: 0.8892\n",
      "Epoch 89/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5546 - accuracy: 0.9024 - val_loss: 0.6800 - val_accuracy: 0.8713\n",
      "Epoch 90/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5579 - accuracy: 0.9011 - val_loss: 0.7101 - val_accuracy: 0.8625\n",
      "Epoch 91/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 22s 57ms/step - loss: 0.5494 - accuracy: 0.9043 - val_loss: 0.6444 - val_accuracy: 0.8808\n",
      "Epoch 92/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.4598 - accuracy: 0.9370 - val_loss: 0.4908 - val_accuracy: 0.9250\n",
      "Epoch 93/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.4144 - accuracy: 0.9514 - val_loss: 0.4758 - val_accuracy: 0.9318\n",
      "Epoch 94/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.3995 - accuracy: 0.9547 - val_loss: 0.4706 - val_accuracy: 0.9335\n",
      "Epoch 95/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.3844 - accuracy: 0.9583 - val_loss: 0.4636 - val_accuracy: 0.9331\n",
      "Epoch 96/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.3786 - accuracy: 0.9583 - val_loss: 0.4614 - val_accuracy: 0.9327\n",
      "Epoch 97/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.3691 - accuracy: 0.9612 - val_loss: 0.4518 - val_accuracy: 0.9358\n",
      "Epoch 98/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.3601 - accuracy: 0.9636 - val_loss: 0.4506 - val_accuracy: 0.9348\n",
      "Epoch 99/180\n",
      "Learning rate: 0.010000000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 24s 61ms/step - loss: 0.3500 - accuracy: 0.9647 - val_loss: 0.4476 - val_accuracy: 0.9369\n",
      "Epoch 100/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.3479 - accuracy: 0.9637 - val_loss: 0.4457 - val_accuracy: 0.9375\n",
      "Epoch 101/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.3406 - accuracy: 0.9658 - val_loss: 0.4385 - val_accuracy: 0.9365\n",
      "Epoch 102/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.3330 - accuracy: 0.9682 - val_loss: 0.4384 - val_accuracy: 0.9370\n",
      "Epoch 103/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.3277 - accuracy: 0.9679 - val_loss: 0.4331 - val_accuracy: 0.9371\n",
      "Epoch 104/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 62ms/step - loss: 0.3207 - accuracy: 0.9690 - val_loss: 0.4272 - val_accuracy: 0.9382\n",
      "Epoch 105/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.3176 - accuracy: 0.9694 - val_loss: 0.4248 - val_accuracy: 0.9383\n",
      "Epoch 106/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.3107 - accuracy: 0.9714 - val_loss: 0.4222 - val_accuracy: 0.9367\n",
      "Epoch 107/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.3051 - accuracy: 0.9708 - val_loss: 0.4257 - val_accuracy: 0.9380\n",
      "Epoch 108/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.3012 - accuracy: 0.9722 - val_loss: 0.4198 - val_accuracy: 0.9383\n",
      "Epoch 109/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.2970 - accuracy: 0.9726 - val_loss: 0.4226 - val_accuracy: 0.9377\n",
      "Epoch 110/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.2943 - accuracy: 0.9720 - val_loss: 0.4173 - val_accuracy: 0.9388\n",
      "Epoch 111/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.2902 - accuracy: 0.9722 - val_loss: 0.4171 - val_accuracy: 0.9381\n",
      "Epoch 112/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.2840 - accuracy: 0.9728 - val_loss: 0.4152 - val_accuracy: 0.9368\n",
      "Epoch 113/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.2792 - accuracy: 0.9745 - val_loss: 0.4127 - val_accuracy: 0.9391\n",
      "Epoch 114/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.2762 - accuracy: 0.9742 - val_loss: 0.4067 - val_accuracy: 0.9405\n",
      "Epoch 115/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 63ms/step - loss: 0.2761 - accuracy: 0.9733 - val_loss: 0.4104 - val_accuracy: 0.9395\n",
      "Epoch 116/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 60ms/step - loss: 0.2704 - accuracy: 0.9744 - val_loss: 0.4003 - val_accuracy: 0.9396\n",
      "Epoch 117/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 60ms/step - loss: 0.2651 - accuracy: 0.9747 - val_loss: 0.4086 - val_accuracy: 0.9370\n",
      "Epoch 118/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.2653 - accuracy: 0.9740 - val_loss: 0.3955 - val_accuracy: 0.9405\n",
      "Epoch 119/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.2583 - accuracy: 0.9766 - val_loss: 0.3950 - val_accuracy: 0.9410\n",
      "Epoch 120/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.2571 - accuracy: 0.9754 - val_loss: 0.3969 - val_accuracy: 0.9380\n",
      "Epoch 121/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.2535 - accuracy: 0.9758 - val_loss: 0.3993 - val_accuracy: 0.9406\n",
      "Epoch 122/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.2512 - accuracy: 0.9756 - val_loss: 0.3911 - val_accuracy: 0.9411\n",
      "Epoch 123/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.2483 - accuracy: 0.9758 - val_loss: 0.3889 - val_accuracy: 0.9418\n",
      "Epoch 124/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.2464 - accuracy: 0.9756 - val_loss: 0.3839 - val_accuracy: 0.9404\n",
      "Epoch 125/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.2430 - accuracy: 0.9765 - val_loss: 0.3900 - val_accuracy: 0.9400\n",
      "Epoch 126/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 25s 62ms/step - loss: 0.2401 - accuracy: 0.9765 - val_loss: 0.3841 - val_accuracy: 0.9406\n",
      "Epoch 127/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.2386 - accuracy: 0.9764 - val_loss: 0.3844 - val_accuracy: 0.9398\n",
      "Epoch 128/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.2346 - accuracy: 0.9766 - val_loss: 0.3814 - val_accuracy: 0.9388\n",
      "Epoch 129/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.2309 - accuracy: 0.9778 - val_loss: 0.3904 - val_accuracy: 0.9369\n",
      "Epoch 130/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.2299 - accuracy: 0.9780 - val_loss: 0.3852 - val_accuracy: 0.9384\n",
      "Epoch 131/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.2270 - accuracy: 0.9779 - val_loss: 0.3770 - val_accuracy: 0.9381\n",
      "Epoch 132/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.2246 - accuracy: 0.9771 - val_loss: 0.3871 - val_accuracy: 0.9375\n",
      "Epoch 133/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.2226 - accuracy: 0.9776 - val_loss: 0.3793 - val_accuracy: 0.9394\n",
      "Epoch 134/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.2205 - accuracy: 0.9778 - val_loss: 0.3839 - val_accuracy: 0.9381\n",
      "Epoch 135/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 61ms/step - loss: 0.2174 - accuracy: 0.9787 - val_loss: 0.3852 - val_accuracy: 0.9372\n",
      "Epoch 136/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.2175 - accuracy: 0.9782 - val_loss: 0.3777 - val_accuracy: 0.9391\n",
      "Epoch 137/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.2181 - accuracy: 0.9766 - val_loss: 0.3770 - val_accuracy: 0.9376\n",
      "Epoch 138/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.2105 - accuracy: 0.9783 - val_loss: 0.3734 - val_accuracy: 0.9409\n",
      "Epoch 139/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.2112 - accuracy: 0.9771 - val_loss: 0.3757 - val_accuracy: 0.9389\n",
      "Epoch 140/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.2098 - accuracy: 0.9774 - val_loss: 0.3718 - val_accuracy: 0.9386\n",
      "Epoch 141/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.2055 - accuracy: 0.9788 - val_loss: 0.3732 - val_accuracy: 0.9367\n",
      "Epoch 142/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.2004 - accuracy: 0.9795 - val_loss: 0.3568 - val_accuracy: 0.9420\n",
      "Epoch 143/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1956 - accuracy: 0.9815 - val_loss: 0.3533 - val_accuracy: 0.9447\n",
      "Epoch 144/180\n",
      "Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1923 - accuracy: 0.9825 - val_loss: 0.3526 - val_accuracy: 0.9447\n",
      "Epoch 145/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1933 - accuracy: 0.9830 - val_loss: 0.3515 - val_accuracy: 0.9446\n",
      "Epoch 146/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1885 - accuracy: 0.9836 - val_loss: 0.3488 - val_accuracy: 0.9448\n",
      "Epoch 147/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1884 - accuracy: 0.9842 - val_loss: 0.3498 - val_accuracy: 0.9451\n",
      "Epoch 148/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1902 - accuracy: 0.9831 - val_loss: 0.3477 - val_accuracy: 0.9458\n",
      "Epoch 149/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1887 - accuracy: 0.9834 - val_loss: 0.3472 - val_accuracy: 0.9466\n",
      "Epoch 150/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1896 - accuracy: 0.9831 - val_loss: 0.3469 - val_accuracy: 0.9468\n",
      "Epoch 151/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1866 - accuracy: 0.9843 - val_loss: 0.3471 - val_accuracy: 0.9456\n",
      "Epoch 152/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1875 - accuracy: 0.9843 - val_loss: 0.3455 - val_accuracy: 0.9470\n",
      "Epoch 153/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 24s 60ms/step - loss: 0.1878 - accuracy: 0.9840 - val_loss: 0.3451 - val_accuracy: 0.9467\n",
      "Epoch 154/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1843 - accuracy: 0.9850 - val_loss: 0.3464 - val_accuracy: 0.9455\n",
      "Epoch 155/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1856 - accuracy: 0.9846 - val_loss: 0.3455 - val_accuracy: 0.9461\n",
      "Epoch 156/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1858 - accuracy: 0.9847 - val_loss: 0.3442 - val_accuracy: 0.9464\n",
      "Epoch 157/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1877 - accuracy: 0.9838 - val_loss: 0.3430 - val_accuracy: 0.9459\n",
      "Epoch 158/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1846 - accuracy: 0.9847 - val_loss: 0.3428 - val_accuracy: 0.9468\n",
      "Epoch 159/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.1866 - accuracy: 0.9838 - val_loss: 0.3429 - val_accuracy: 0.9469\n",
      "Epoch 160/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.1834 - accuracy: 0.9842 - val_loss: 0.3439 - val_accuracy: 0.9470\n",
      "Epoch 161/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.1845 - accuracy: 0.9846 - val_loss: 0.3416 - val_accuracy: 0.9467\n",
      "Epoch 162/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.1842 - accuracy: 0.9840 - val_loss: 0.3435 - val_accuracy: 0.9463\n",
      "Epoch 163/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1827 - accuracy: 0.9846 - val_loss: 0.3427 - val_accuracy: 0.9464\n",
      "Epoch 164/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1832 - accuracy: 0.9840 - val_loss: 0.3419 - val_accuracy: 0.9457\n",
      "Epoch 165/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.1805 - accuracy: 0.9856 - val_loss: 0.3417 - val_accuracy: 0.9463\n",
      "Epoch 166/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 0.1804 - accuracy: 0.9853 - val_loss: 0.3407 - val_accuracy: 0.9466\n",
      "Epoch 167/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1805 - accuracy: 0.9848 - val_loss: 0.3416 - val_accuracy: 0.9464\n",
      "Epoch 168/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1829 - accuracy: 0.9837 - val_loss: 0.3405 - val_accuracy: 0.9472\n",
      "Epoch 169/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1830 - accuracy: 0.9845 - val_loss: 0.3399 - val_accuracy: 0.9474\n",
      "Epoch 170/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1785 - accuracy: 0.9860 - val_loss: 0.3416 - val_accuracy: 0.9466\n",
      "Epoch 171/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1787 - accuracy: 0.9857 - val_loss: 0.3407 - val_accuracy: 0.9470\n",
      "Epoch 172/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1786 - accuracy: 0.9856 - val_loss: 0.3422 - val_accuracy: 0.9478\n",
      "Epoch 173/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1782 - accuracy: 0.9853 - val_loss: 0.3415 - val_accuracy: 0.9472\n",
      "Epoch 174/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1794 - accuracy: 0.9850 - val_loss: 0.3411 - val_accuracy: 0.9463\n",
      "Epoch 175/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1767 - accuracy: 0.9861 - val_loss: 0.3412 - val_accuracy: 0.9454\n",
      "Epoch 176/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1774 - accuracy: 0.9856 - val_loss: 0.3401 - val_accuracy: 0.9461\n",
      "Epoch 177/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1791 - accuracy: 0.9850 - val_loss: 0.3391 - val_accuracy: 0.9456\n",
      "Epoch 178/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1780 - accuracy: 0.9855 - val_loss: 0.3391 - val_accuracy: 0.9462\n",
      "Epoch 179/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1777 - accuracy: 0.9850 - val_loss: 0.3382 - val_accuracy: 0.9459\n",
      "Epoch 180/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1746 - accuracy: 0.9865 - val_loss: 0.3409 - val_accuracy: 0.9448\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
    "callbacks = [lr_scheduler]\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=4,\n",
    "                             height_shift_range=4,\n",
    "                             horizontal_flip=True,\n",
    "                             preprocessing_function=get_random_eraser(pixel_level=True))\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=epochs, workers=4,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa6929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
