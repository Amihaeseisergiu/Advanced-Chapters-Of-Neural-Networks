{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a5057a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "References:\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "https://arxiv.org/pdf/1604.04112v4.pdf\n",
    "https://github.com/yu4u/cutout-random-erasing\n",
    "\"\"\"\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.layers import Add, Dense, Conv2D, BatchNormalization\n",
    "from keras.layers import Activation, AveragePooling2D, Input, Flatten\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e34ed2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 180\n",
    "n_classes = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30c65c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0eb79e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet:\n",
    "    def __init__(self, size=44, stacks=3, starting_filter=16):\n",
    "        self.size = size\n",
    "        self.stacks = stacks\n",
    "        self.starting_filter = starting_filter\n",
    "        self.residual_blocks = (size - 2) // 6\n",
    "        \n",
    "    def get_model(self, input_shape=(32, 32, 3), n_classes=10):\n",
    "        n_filters = self.starting_filter\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        network = self.layer(inputs, n_filters)\n",
    "        network = self.stack(network, n_filters, True)\n",
    "\n",
    "        for _ in range(self.stacks - 1):\n",
    "            n_filters *= 2\n",
    "            network = self.stack(network, n_filters)\n",
    "\n",
    "        network = Activation('elu')(network)\n",
    "        network = AveragePooling2D(pool_size=network.shape[1])(network)\n",
    "        network = Flatten()(network)\n",
    "        outputs = Dense(n_classes, activation='softmax', \n",
    "                        kernel_initializer='he_normal')(network)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def stack(self, inputs, n_filters, first_stack=False):\n",
    "        stack = inputs\n",
    "\n",
    "        if first_stack:\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "        else:\n",
    "            stack = self.convolution_block(stack, n_filters)\n",
    "\n",
    "        for _ in range(self.residual_blocks - 1):\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "\n",
    "        return stack\n",
    "    \n",
    "    def identity_block(self, inputs, n_filters):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "\n",
    "    def convolution_block(self, inputs, n_filters, strides=2):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, strides=strides,\n",
    "                           normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        shortcut = self.layer(shortcut, n_filters,\n",
    "                              kernel_size=1, strides=strides,\n",
    "                              activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "    \n",
    "    def layer(self, inputs, n_filters, kernel_size=3,\n",
    "              strides=1, activation='elu', normalize_batch=True):\n",
    "    \n",
    "        convolution = Conv2D(n_filters, kernel_size=kernel_size,\n",
    "                             strides=strides, padding='same',\n",
    "                             kernel_initializer=\"he_normal\",\n",
    "                             kernel_regularizer=l2(1e-4))\n",
    "\n",
    "        x = convolution(inputs)\n",
    "\n",
    "        if normalize_batch:\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def learning_rate_schedule(epoch):\n",
    "    new_learning_rate = learning_rate\n",
    "\n",
    "    if epoch <= 90:\n",
    "        pass\n",
    "    elif epoch > 90 and epoch <= 140:\n",
    "        new_learning_rate = learning_rate * 0.1\n",
    "    else:\n",
    "        new_learning_rate = learning_rate * 0.01\n",
    "        \n",
    "    print('Learning rate:', new_learning_rate)\n",
    "    \n",
    "    return new_learning_rate\n",
    "\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        if input_img.ndim == 3:\n",
    "            img_h, img_w, img_c = input_img.shape\n",
    "        elif input_img.ndim == 2:\n",
    "            img_h, img_w = input_img.shape\n",
    "\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            if input_img.ndim == 3:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "            if input_img.ndim == 2:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3de5a3fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 32, 32, 16)   448         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 32, 32, 16)   64          conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 32, 32, 16)   0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 32, 32, 16)   2320        activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 32, 32, 16)   0           conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 32, 32, 16)   2320        activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 32, 32, 16)   64          conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_63 (Add)                    (None, 32, 32, 16)   0           activation_69[0][0]              \n",
      "                                                                 batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 32, 32, 16)   2320        add_63[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 32, 32, 16)   0           conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 32, 32, 16)   2320        activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 32, 32, 16)   64          conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_64 (Add)                    (None, 32, 32, 16)   0           add_63[0][0]                     \n",
      "                                                                 batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 32, 32, 16)   2320        add_64[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 32, 32, 16)   0           conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 32, 32, 16)   2320        activation_72[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 32, 32, 16)   64          conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_65 (Add)                    (None, 32, 32, 16)   0           add_64[0][0]                     \n",
      "                                                                 batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 32, 32, 16)   2320        add_65[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 32, 32, 16)   0           conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 32, 32, 16)   2320        activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 32, 32, 16)   64          conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_66 (Add)                    (None, 32, 32, 16)   0           add_65[0][0]                     \n",
      "                                                                 batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 32, 32, 16)   2320        add_66[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 32, 32, 16)   0           conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 32, 32, 16)   2320        activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 32, 32, 16)   64          conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_67 (Add)                    (None, 32, 32, 16)   0           add_66[0][0]                     \n",
      "                                                                 batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 32, 32, 16)   2320        add_67[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 32, 32, 16)   0           conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 32, 32, 16)   2320        activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 32, 32, 16)   64          conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_68 (Add)                    (None, 32, 32, 16)   0           add_67[0][0]                     \n",
      "                                                                 batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 32, 32, 16)   2320        add_68[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 32, 32, 16)   0           conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 32, 32, 16)   2320        activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 32, 32, 16)   64          conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_69 (Add)                    (None, 32, 32, 16)   0           add_68[0][0]                     \n",
      "                                                                 batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 16, 16, 32)   4640        add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 16, 16, 32)   0           conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 16, 16, 32)   544         add_69[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 16, 16, 32)   9248        activation_77[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 16, 16, 32)   128         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 16, 16, 32)   128         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_70 (Add)                    (None, 16, 16, 32)   0           batch_normalization_81[0][0]     \n",
      "                                                                 batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 16, 16, 32)   9248        add_70[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 16, 16, 32)   0           conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 16, 16, 32)   9248        activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 16, 16, 32)   128         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 16, 16, 32)   0           add_70[0][0]                     \n",
      "                                                                 batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 16, 16, 32)   9248        add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 16, 16, 32)   0           conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 16, 16, 32)   9248        activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 16, 16, 32)   128         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 16, 16, 32)   0           add_71[0][0]                     \n",
      "                                                                 batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 16, 16, 32)   9248        add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 16, 16, 32)   0           conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 16, 16, 32)   9248        activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 16, 16, 32)   128         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 16, 16, 32)   0           add_72[0][0]                     \n",
      "                                                                 batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 16, 16, 32)   9248        add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 16, 16, 32)   0           conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 16, 16, 32)   9248        activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 16, 16, 32)   128         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 16, 16, 32)   0           add_73[0][0]                     \n",
      "                                                                 batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 16, 16, 32)   9248        add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 16, 16, 32)   0           conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 16, 16, 32)   9248        activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 16, 16, 32)   128         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 16, 16, 32)   0           add_74[0][0]                     \n",
      "                                                                 batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 16, 16, 32)   9248        add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 16, 16, 32)   0           conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 16, 16, 32)   9248        activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 16, 16, 32)   128         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_76 (Add)                    (None, 16, 16, 32)   0           add_75[0][0]                     \n",
      "                                                                 batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 8, 8, 64)     18496       add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 64)     0           conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 8, 8, 64)     2112        add_76[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 8, 8, 64)     36928       activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 64)     256         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 64)     256         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 8, 8, 64)     0           batch_normalization_89[0][0]     \n",
      "                                                                 batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 8, 8, 64)     36928       add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 64)     0           conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 8, 8, 64)     36928       activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 64)     256         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_78 (Add)                    (None, 8, 8, 64)     0           add_77[0][0]                     \n",
      "                                                                 batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 8, 8, 64)     36928       add_78[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 64)     0           conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 8, 8, 64)     36928       activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 64)     256         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_79 (Add)                    (None, 8, 8, 64)     0           add_78[0][0]                     \n",
      "                                                                 batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 8, 8, 64)     36928       add_79[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 64)     0           conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 8, 8, 64)     36928       activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 64)     256         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_80 (Add)                    (None, 8, 8, 64)     0           add_79[0][0]                     \n",
      "                                                                 batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 8, 8, 64)     36928       add_80[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 64)     0           conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 8, 8, 64)     36928       activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 64)     256         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_81 (Add)                    (None, 8, 8, 64)     0           add_80[0][0]                     \n",
      "                                                                 batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 8, 8, 64)     36928       add_81[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 64)     0           conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 8, 8, 64)     36928       activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 64)     256         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_82 (Add)                    (None, 8, 8, 64)     0           add_81[0][0]                     \n",
      "                                                                 batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 8, 8, 64)     36928       add_82[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 64)     0           conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 8, 8, 64)     36928       activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 8, 8, 64)     256         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_83 (Add)                    (None, 8, 8, 64)     0           add_82[0][0]                     \n",
      "                                                                 batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 64)     0           add_83[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 1, 1, 64)     0           activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 64)           0           average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           650         flatten_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 663,242\n",
      "Trainable params: 661,450\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet = Resnet()\n",
    "\n",
    "model = resnet.get_model()\n",
    "\n",
    "optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22558699",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 30s 66ms/step - loss: 2.1054 - accuracy: 0.3555 - val_loss: 1.8543 - val_accuracy: 0.4416\n",
      "Epoch 2/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 1.6766 - accuracy: 0.5070 - val_loss: 1.7795 - val_accuracy: 0.4819\n",
      "Epoch 3/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 1.4538 - accuracy: 0.5816 - val_loss: 1.2391 - val_accuracy: 0.6563\n",
      "Epoch 4/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 1.3039 - accuracy: 0.6334 - val_loss: 1.4266 - val_accuracy: 0.5979\n",
      "Epoch 5/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 1.1892 - accuracy: 0.6694 - val_loss: 1.1024 - val_accuracy: 0.7098\n",
      "Epoch 6/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 1.0812 - accuracy: 0.7051 - val_loss: 1.1525 - val_accuracy: 0.6953\n",
      "Epoch 7/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 1.0240 - accuracy: 0.7226 - val_loss: 0.9883 - val_accuracy: 0.7492\n",
      "Epoch 8/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.9673 - accuracy: 0.7407 - val_loss: 0.8868 - val_accuracy: 0.7728\n",
      "Epoch 9/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.9152 - accuracy: 0.7598 - val_loss: 0.9358 - val_accuracy: 0.7613\n",
      "Epoch 10/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.8842 - accuracy: 0.7710 - val_loss: 1.1586 - val_accuracy: 0.7036\n",
      "Epoch 11/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.8508 - accuracy: 0.7809 - val_loss: 0.7997 - val_accuracy: 0.8043\n",
      "Epoch 12/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.8288 - accuracy: 0.7892 - val_loss: 0.7653 - val_accuracy: 0.8055\n",
      "Epoch 13/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.8056 - accuracy: 0.7979 - val_loss: 0.9261 - val_accuracy: 0.7557\n",
      "Epoch 14/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.7879 - accuracy: 0.8032 - val_loss: 0.7151 - val_accuracy: 0.8364\n",
      "Epoch 15/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.7784 - accuracy: 0.8071 - val_loss: 0.7106 - val_accuracy: 0.8345\n",
      "Epoch 16/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.7518 - accuracy: 0.8161 - val_loss: 0.7117 - val_accuracy: 0.8329\n",
      "Epoch 17/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.7478 - accuracy: 0.8161 - val_loss: 0.7733 - val_accuracy: 0.8188\n",
      "Epoch 18/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.7480 - accuracy: 0.8167 - val_loss: 0.6563 - val_accuracy: 0.8538\n",
      "Epoch 19/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.7249 - accuracy: 0.8263 - val_loss: 1.1538 - val_accuracy: 0.6865\n",
      "Epoch 20/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.7114 - accuracy: 0.8312 - val_loss: 0.6711 - val_accuracy: 0.8484\n",
      "Epoch 21/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.7151 - accuracy: 0.8289 - val_loss: 0.6925 - val_accuracy: 0.8451\n",
      "Epoch 22/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.7107 - accuracy: 0.8319 - val_loss: 0.7261 - val_accuracy: 0.8425\n",
      "Epoch 23/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.7044 - accuracy: 0.8342 - val_loss: 0.6923 - val_accuracy: 0.8452\n",
      "Epoch 24/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.6959 - accuracy: 0.8396 - val_loss: 0.9375 - val_accuracy: 0.7535\n",
      "Epoch 25/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6945 - accuracy: 0.8396 - val_loss: 1.3404 - val_accuracy: 0.6269\n",
      "Epoch 26/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6913 - accuracy: 0.8426 - val_loss: 0.6572 - val_accuracy: 0.8633\n",
      "Epoch 27/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6814 - accuracy: 0.8437 - val_loss: 0.6457 - val_accuracy: 0.8616\n",
      "Epoch 28/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6842 - accuracy: 0.8443 - val_loss: 0.8515 - val_accuracy: 0.7952\n",
      "Epoch 29/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6776 - accuracy: 0.8468 - val_loss: 0.6681 - val_accuracy: 0.8603\n",
      "Epoch 30/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6785 - accuracy: 0.8463 - val_loss: 0.6422 - val_accuracy: 0.8671\n",
      "Epoch 31/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6736 - accuracy: 0.8500 - val_loss: 0.7230 - val_accuracy: 0.8439\n",
      "Epoch 32/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.6651 - accuracy: 0.8539 - val_loss: 0.7025 - val_accuracy: 0.8415\n",
      "Epoch 33/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6646 - accuracy: 0.8537 - val_loss: 0.6187 - val_accuracy: 0.8741\n",
      "Epoch 34/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6711 - accuracy: 0.8543 - val_loss: 0.6656 - val_accuracy: 0.8611\n",
      "Epoch 35/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6621 - accuracy: 0.8578 - val_loss: 0.6989 - val_accuracy: 0.8482\n",
      "Epoch 36/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6615 - accuracy: 0.8562 - val_loss: 0.7626 - val_accuracy: 0.8289\n",
      "Epoch 37/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6574 - accuracy: 0.8589 - val_loss: 0.7306 - val_accuracy: 0.8511\n",
      "Epoch 38/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.6559 - accuracy: 0.8597 - val_loss: 0.8276 - val_accuracy: 0.8151\n",
      "Epoch 39/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6571 - accuracy: 0.8596 - val_loss: 0.9238 - val_accuracy: 0.7790\n",
      "Epoch 40/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6507 - accuracy: 0.8619 - val_loss: 0.7084 - val_accuracy: 0.8536\n",
      "Epoch 41/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6529 - accuracy: 0.8624 - val_loss: 0.8781 - val_accuracy: 0.7876\n",
      "Epoch 42/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6537 - accuracy: 0.8618 - val_loss: 0.6963 - val_accuracy: 0.8508\n",
      "Epoch 43/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6491 - accuracy: 0.8641 - val_loss: 0.6742 - val_accuracy: 0.8641\n",
      "Epoch 44/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6504 - accuracy: 0.8638 - val_loss: 0.6354 - val_accuracy: 0.8690\n",
      "Epoch 45/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6485 - accuracy: 0.8644 - val_loss: 0.6445 - val_accuracy: 0.8672\n",
      "Epoch 46/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6497 - accuracy: 0.8638 - val_loss: 0.6775 - val_accuracy: 0.8671\n",
      "Epoch 47/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6525 - accuracy: 0.8661 - val_loss: 0.6952 - val_accuracy: 0.8604\n",
      "Epoch 48/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6470 - accuracy: 0.8674 - val_loss: 0.6408 - val_accuracy: 0.8755\n",
      "Epoch 49/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 67ms/step - loss: 0.6452 - accuracy: 0.8659 - val_loss: 0.7651 - val_accuracy: 0.8496\n",
      "Epoch 50/180\n",
      "Learning rate: 0.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6497 - accuracy: 0.8650 - val_loss: 0.6287 - val_accuracy: 0.8777\n",
      "Epoch 51/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6388 - accuracy: 0.8691 - val_loss: 0.6443 - val_accuracy: 0.8691\n",
      "Epoch 52/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 67ms/step - loss: 0.6429 - accuracy: 0.8679 - val_loss: 0.6347 - val_accuracy: 0.8785\n",
      "Epoch 53/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6389 - accuracy: 0.8683 - val_loss: 0.6642 - val_accuracy: 0.8731\n",
      "Epoch 54/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6421 - accuracy: 0.8699 - val_loss: 0.6511 - val_accuracy: 0.8697\n",
      "Epoch 55/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.6403 - accuracy: 0.8704 - val_loss: 0.6784 - val_accuracy: 0.8649\n",
      "Epoch 56/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6373 - accuracy: 0.8720 - val_loss: 0.6230 - val_accuracy: 0.8866\n",
      "Epoch 57/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.6370 - accuracy: 0.8711 - val_loss: 0.6207 - val_accuracy: 0.8801\n",
      "Epoch 58/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6335 - accuracy: 0.8738 - val_loss: 0.8850 - val_accuracy: 0.7930\n",
      "Epoch 59/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6358 - accuracy: 0.8736 - val_loss: 0.8647 - val_accuracy: 0.8185\n",
      "Epoch 60/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6294 - accuracy: 0.8739 - val_loss: 0.6513 - val_accuracy: 0.8758\n",
      "Epoch 61/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6345 - accuracy: 0.8724 - val_loss: 0.6753 - val_accuracy: 0.8690\n",
      "Epoch 62/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6308 - accuracy: 0.8747 - val_loss: 0.6151 - val_accuracy: 0.8868\n",
      "Epoch 63/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6322 - accuracy: 0.8741 - val_loss: 0.9318 - val_accuracy: 0.7988\n",
      "Epoch 64/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6361 - accuracy: 0.8753 - val_loss: 0.6261 - val_accuracy: 0.8807\n",
      "Epoch 65/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6314 - accuracy: 0.8748 - val_loss: 0.7039 - val_accuracy: 0.8554\n",
      "Epoch 66/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6309 - accuracy: 0.8749 - val_loss: 0.7125 - val_accuracy: 0.8571\n",
      "Epoch 67/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6251 - accuracy: 0.8786 - val_loss: 0.7433 - val_accuracy: 0.8448\n",
      "Epoch 68/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6323 - accuracy: 0.8767 - val_loss: 0.6941 - val_accuracy: 0.8710\n",
      "Epoch 69/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.6297 - accuracy: 0.8767 - val_loss: 0.6428 - val_accuracy: 0.8818\n",
      "Epoch 70/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6315 - accuracy: 0.8756 - val_loss: 0.7375 - val_accuracy: 0.8483\n",
      "Epoch 71/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6287 - accuracy: 0.8760 - val_loss: 0.6798 - val_accuracy: 0.8706\n",
      "Epoch 72/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6250 - accuracy: 0.8778 - val_loss: 0.6378 - val_accuracy: 0.8803\n",
      "Epoch 73/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6249 - accuracy: 0.8783 - val_loss: 0.6117 - val_accuracy: 0.8861\n",
      "Epoch 74/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6273 - accuracy: 0.8793 - val_loss: 0.8271 - val_accuracy: 0.8251\n",
      "Epoch 75/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6248 - accuracy: 0.8786 - val_loss: 0.7337 - val_accuracy: 0.8593\n",
      "Epoch 76/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 68ms/step - loss: 0.6222 - accuracy: 0.8805 - val_loss: 0.6576 - val_accuracy: 0.8714\n",
      "Epoch 77/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.6286 - accuracy: 0.8776 - val_loss: 0.6740 - val_accuracy: 0.8767\n",
      "Epoch 78/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6265 - accuracy: 0.8776 - val_loss: 0.7252 - val_accuracy: 0.8563\n",
      "Epoch 79/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6268 - accuracy: 0.8796 - val_loss: 0.6338 - val_accuracy: 0.8826\n",
      "Epoch 80/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6127 - accuracy: 0.8830 - val_loss: 0.7362 - val_accuracy: 0.8534\n",
      "Epoch 81/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 26s 67ms/step - loss: 0.6217 - accuracy: 0.8796 - val_loss: 0.6255 - val_accuracy: 0.8857\n",
      "Epoch 82/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.6222 - accuracy: 0.8799 - val_loss: 0.7135 - val_accuracy: 0.8593\n",
      "Epoch 83/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.6198 - accuracy: 0.8803 - val_loss: 0.6816 - val_accuracy: 0.8698\n",
      "Epoch 84/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.6245 - accuracy: 0.8793 - val_loss: 0.6154 - val_accuracy: 0.8928\n",
      "Epoch 85/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.6201 - accuracy: 0.8799 - val_loss: 0.6112 - val_accuracy: 0.8885\n",
      "Epoch 86/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.6245 - accuracy: 0.8788 - val_loss: 0.6229 - val_accuracy: 0.8826\n",
      "Epoch 87/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.6124 - accuracy: 0.8834 - val_loss: 0.6550 - val_accuracy: 0.8813\n",
      "Epoch 88/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.6240 - accuracy: 0.8809 - val_loss: 0.6481 - val_accuracy: 0.8817\n",
      "Epoch 89/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.6233 - accuracy: 0.8789 - val_loss: 1.2003 - val_accuracy: 0.7005\n",
      "Epoch 90/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.6143 - accuracy: 0.8837 - val_loss: 0.6720 - val_accuracy: 0.8721\n",
      "Epoch 91/180\n",
      "Learning rate: 0.1\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.6213 - accuracy: 0.8810 - val_loss: 0.6273 - val_accuracy: 0.8903\n",
      "Epoch 92/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.5256 - accuracy: 0.9136 - val_loss: 0.4955 - val_accuracy: 0.9275\n",
      "Epoch 93/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.4678 - accuracy: 0.9337 - val_loss: 0.4859 - val_accuracy: 0.9297\n",
      "Epoch 94/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.4566 - accuracy: 0.9351 - val_loss: 0.4735 - val_accuracy: 0.9331\n",
      "Epoch 95/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.4415 - accuracy: 0.9388 - val_loss: 0.4652 - val_accuracy: 0.9334\n",
      "Epoch 96/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.4300 - accuracy: 0.9414 - val_loss: 0.4615 - val_accuracy: 0.9355\n",
      "Epoch 97/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.4211 - accuracy: 0.9433 - val_loss: 0.4518 - val_accuracy: 0.9359\n",
      "Epoch 98/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 30s 75ms/step - loss: 0.4092 - accuracy: 0.9470 - val_loss: 0.4518 - val_accuracy: 0.9375\n",
      "Epoch 99/180\n",
      "Learning rate: 0.010000000000000002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 28s 70ms/step - loss: 0.4056 - accuracy: 0.9470 - val_loss: 0.4430 - val_accuracy: 0.9363\n",
      "Epoch 100/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.3960 - accuracy: 0.9491 - val_loss: 0.4389 - val_accuracy: 0.9376\n",
      "Epoch 101/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3880 - accuracy: 0.9497 - val_loss: 0.4315 - val_accuracy: 0.9386\n",
      "Epoch 102/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3843 - accuracy: 0.9510 - val_loss: 0.4278 - val_accuracy: 0.9397\n",
      "Epoch 103/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3770 - accuracy: 0.9514 - val_loss: 0.4297 - val_accuracy: 0.9393\n",
      "Epoch 104/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3698 - accuracy: 0.9525 - val_loss: 0.4279 - val_accuracy: 0.9382\n",
      "Epoch 105/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3650 - accuracy: 0.9523 - val_loss: 0.4140 - val_accuracy: 0.9415\n",
      "Epoch 106/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3639 - accuracy: 0.9536 - val_loss: 0.4189 - val_accuracy: 0.9400\n",
      "Epoch 107/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.3565 - accuracy: 0.9530 - val_loss: 0.4124 - val_accuracy: 0.9398\n",
      "Epoch 108/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.3505 - accuracy: 0.9553 - val_loss: 0.4057 - val_accuracy: 0.9413\n",
      "Epoch 109/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.3445 - accuracy: 0.9573 - val_loss: 0.4053 - val_accuracy: 0.9411\n",
      "Epoch 110/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.3430 - accuracy: 0.9561 - val_loss: 0.4086 - val_accuracy: 0.9399\n",
      "Epoch 111/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3384 - accuracy: 0.9558 - val_loss: 0.4037 - val_accuracy: 0.9411\n",
      "Epoch 112/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.3300 - accuracy: 0.9574 - val_loss: 0.4013 - val_accuracy: 0.9410\n",
      "Epoch 113/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3269 - accuracy: 0.9580 - val_loss: 0.4048 - val_accuracy: 0.9395\n",
      "Epoch 114/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.3265 - accuracy: 0.9575 - val_loss: 0.3978 - val_accuracy: 0.9390\n",
      "Epoch 115/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.3197 - accuracy: 0.9587 - val_loss: 0.3885 - val_accuracy: 0.9395\n",
      "Epoch 116/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.3163 - accuracy: 0.9592 - val_loss: 0.3967 - val_accuracy: 0.9390\n",
      "Epoch 117/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.3090 - accuracy: 0.9612 - val_loss: 0.3976 - val_accuracy: 0.9387\n",
      "Epoch 118/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3069 - accuracy: 0.9608 - val_loss: 0.3961 - val_accuracy: 0.9386\n",
      "Epoch 119/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3062 - accuracy: 0.9598 - val_loss: 0.3914 - val_accuracy: 0.9400\n",
      "Epoch 120/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.3028 - accuracy: 0.9611 - val_loss: 0.3749 - val_accuracy: 0.9426\n",
      "Epoch 121/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2967 - accuracy: 0.9624 - val_loss: 0.3857 - val_accuracy: 0.9387\n",
      "Epoch 122/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 30s 75ms/step - loss: 0.2971 - accuracy: 0.9608 - val_loss: 0.3827 - val_accuracy: 0.9404\n",
      "Epoch 123/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2917 - accuracy: 0.9619 - val_loss: 0.3871 - val_accuracy: 0.9381\n",
      "Epoch 124/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2922 - accuracy: 0.9613 - val_loss: 0.3812 - val_accuracy: 0.9414\n",
      "Epoch 125/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2882 - accuracy: 0.9620 - val_loss: 0.3790 - val_accuracy: 0.9385\n",
      "Epoch 126/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2841 - accuracy: 0.9627 - val_loss: 0.3699 - val_accuracy: 0.9382\n",
      "Epoch 127/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2828 - accuracy: 0.9622 - val_loss: 0.3818 - val_accuracy: 0.9361\n",
      "Epoch 128/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2786 - accuracy: 0.9628 - val_loss: 0.3776 - val_accuracy: 0.9390\n",
      "Epoch 129/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2777 - accuracy: 0.9622 - val_loss: 0.3679 - val_accuracy: 0.9415\n",
      "Epoch 130/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2729 - accuracy: 0.9633 - val_loss: 0.3735 - val_accuracy: 0.9397\n",
      "Epoch 131/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2746 - accuracy: 0.9618 - val_loss: 0.3664 - val_accuracy: 0.9391\n",
      "Epoch 132/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2719 - accuracy: 0.9627 - val_loss: 0.3910 - val_accuracy: 0.9327\n",
      "Epoch 133/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2683 - accuracy: 0.9634 - val_loss: 0.3713 - val_accuracy: 0.9386\n",
      "Epoch 134/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2664 - accuracy: 0.9631 - val_loss: 0.3712 - val_accuracy: 0.9376\n",
      "Epoch 135/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2647 - accuracy: 0.9632 - val_loss: 0.3691 - val_accuracy: 0.9386\n",
      "Epoch 136/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2604 - accuracy: 0.9646 - val_loss: 0.3637 - val_accuracy: 0.9377\n",
      "Epoch 137/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2601 - accuracy: 0.9636 - val_loss: 0.3727 - val_accuracy: 0.9381\n",
      "Epoch 138/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2577 - accuracy: 0.9643 - val_loss: 0.3689 - val_accuracy: 0.9385\n",
      "Epoch 139/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2559 - accuracy: 0.9642 - val_loss: 0.3753 - val_accuracy: 0.9337\n",
      "Epoch 140/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2526 - accuracy: 0.9646 - val_loss: 0.3682 - val_accuracy: 0.9374\n",
      "Epoch 141/180\n",
      "Learning rate: 0.010000000000000002\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2518 - accuracy: 0.9639 - val_loss: 0.3668 - val_accuracy: 0.9380\n",
      "Epoch 142/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2424 - accuracy: 0.9679 - val_loss: 0.3424 - val_accuracy: 0.9446\n",
      "Epoch 143/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2338 - accuracy: 0.9714 - val_loss: 0.3422 - val_accuracy: 0.9445\n",
      "Epoch 144/180\n",
      "Learning rate: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2311 - accuracy: 0.9721 - val_loss: 0.3370 - val_accuracy: 0.9459\n",
      "Epoch 145/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2247 - accuracy: 0.9748 - val_loss: 0.3367 - val_accuracy: 0.9456\n",
      "Epoch 146/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2281 - accuracy: 0.9729 - val_loss: 0.3351 - val_accuracy: 0.9453\n",
      "Epoch 147/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 0.2253 - accuracy: 0.9746 - val_loss: 0.3362 - val_accuracy: 0.9468\n",
      "Epoch 148/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2237 - accuracy: 0.9745 - val_loss: 0.3355 - val_accuracy: 0.9463\n",
      "Epoch 149/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2211 - accuracy: 0.9750 - val_loss: 0.3353 - val_accuracy: 0.9462\n",
      "Epoch 150/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2184 - accuracy: 0.9756 - val_loss: 0.3343 - val_accuracy: 0.9460\n",
      "Epoch 151/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2199 - accuracy: 0.9753 - val_loss: 0.3328 - val_accuracy: 0.9459\n",
      "Epoch 152/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2205 - accuracy: 0.9752 - val_loss: 0.3348 - val_accuracy: 0.9460\n",
      "Epoch 153/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2195 - accuracy: 0.9752 - val_loss: 0.3324 - val_accuracy: 0.9467\n",
      "Epoch 154/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2212 - accuracy: 0.9747 - val_loss: 0.3318 - val_accuracy: 0.9469\n",
      "Epoch 155/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2172 - accuracy: 0.9760 - val_loss: 0.3324 - val_accuracy: 0.9459\n",
      "Epoch 156/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2186 - accuracy: 0.9749 - val_loss: 0.3324 - val_accuracy: 0.9469\n",
      "Epoch 157/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2150 - accuracy: 0.9769 - val_loss: 0.3307 - val_accuracy: 0.9477\n",
      "Epoch 158/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2140 - accuracy: 0.9777 - val_loss: 0.3304 - val_accuracy: 0.9475\n",
      "Epoch 159/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2157 - accuracy: 0.9760 - val_loss: 0.3309 - val_accuracy: 0.9481\n",
      "Epoch 160/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2131 - accuracy: 0.9773 - val_loss: 0.3308 - val_accuracy: 0.9475\n",
      "Epoch 161/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2145 - accuracy: 0.9760 - val_loss: 0.3293 - val_accuracy: 0.9466\n",
      "Epoch 162/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2151 - accuracy: 0.9761 - val_loss: 0.3299 - val_accuracy: 0.9461\n",
      "Epoch 163/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2129 - accuracy: 0.9771 - val_loss: 0.3299 - val_accuracy: 0.9460\n",
      "Epoch 164/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2096 - accuracy: 0.9780 - val_loss: 0.3314 - val_accuracy: 0.9447\n",
      "Epoch 165/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2136 - accuracy: 0.9763 - val_loss: 0.3320 - val_accuracy: 0.9451\n",
      "Epoch 166/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2098 - accuracy: 0.9777 - val_loss: 0.3295 - val_accuracy: 0.9466\n",
      "Epoch 167/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2120 - accuracy: 0.9768 - val_loss: 0.3288 - val_accuracy: 0.9453\n",
      "Epoch 168/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.2129 - accuracy: 0.9767 - val_loss: 0.3290 - val_accuracy: 0.9462\n",
      "Epoch 169/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 29s 72ms/step - loss: 0.2099 - accuracy: 0.9773 - val_loss: 0.3286 - val_accuracy: 0.9475\n",
      "Epoch 170/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2075 - accuracy: 0.9778 - val_loss: 0.3272 - val_accuracy: 0.9474\n",
      "Epoch 171/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2104 - accuracy: 0.9770 - val_loss: 0.3291 - val_accuracy: 0.9467\n",
      "Epoch 172/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2057 - accuracy: 0.9783 - val_loss: 0.3280 - val_accuracy: 0.9463\n",
      "Epoch 173/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.2078 - accuracy: 0.9778 - val_loss: 0.3288 - val_accuracy: 0.9470\n",
      "Epoch 174/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2076 - accuracy: 0.9776 - val_loss: 0.3288 - val_accuracy: 0.9469\n",
      "Epoch 175/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.2095 - accuracy: 0.9773 - val_loss: 0.3298 - val_accuracy: 0.9453\n",
      "Epoch 176/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2084 - accuracy: 0.9775 - val_loss: 0.3304 - val_accuracy: 0.9465\n",
      "Epoch 177/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2077 - accuracy: 0.9777 - val_loss: 0.3289 - val_accuracy: 0.9471\n",
      "Epoch 178/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2074 - accuracy: 0.9779 - val_loss: 0.3284 - val_accuracy: 0.9467\n",
      "Epoch 179/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2054 - accuracy: 0.9783 - val_loss: 0.3284 - val_accuracy: 0.9463\n",
      "Epoch 180/180\n",
      "Learning rate: 0.001\n",
      "391/391 [==============================] - 28s 70ms/step - loss: 0.2067 - accuracy: 0.9775 - val_loss: 0.3290 - val_accuracy: 0.9464\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
    "callbacks = [lr_scheduler]\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=4,\n",
    "                             height_shift_range=4,\n",
    "                             horizontal_flip=True,\n",
    "                             preprocessing_function=get_random_eraser(p=1, pixel_level=True))\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=epochs, workers=4,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa6929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
